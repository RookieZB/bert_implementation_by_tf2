{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"text_layout.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"display_name":"Python 3","name":"python3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"Gsi09JORaOMT"},"source":["Check the outputs of LayoutLMv2 implementation."]},{"cell_type":"code","metadata":{"id":"71eYTZ6XQycm"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3Ov04jWvmxZA"},"source":["!pip install tensorflow-addons\n","!pip install transformers\n","!pip install pytesseract\n","!pip install tesseract\n","!pip install pyyaml==5.1\n","!pip install detectron2 -f https://dl.fbaipublicfiles.com/detectron2/wheels/cu102/torch1.9/index.html\n","!sudo apt update\n","!sudo apt install tesseract-ocr\n","!sudo apt install libtesseract-dev\n","!git clone https://github.com/RookieZB/bert_implementation_by_tf2.git\n","!wget https://huggingface.co/microsoft/layoutlmv2-base-uncased/resolve/main/pytorch_model.bin"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5YOSGgIRL8mH"},"source":["import os\n","import time\n","import numpy as np\n","import tensorflow as tf\n","from PIL import Image\n","from transformers import LayoutLMv2Processor, LayoutLMv2Model\n","from bert_implementation_by_tf2.modules import layoutlm"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xIsshOV1L8in"},"source":["doc_image = Image.open('./drive/My Drive/Python/Research/tasks/datasets/doc/test.PNG').convert('RGB')\n","layout_torch = LayoutLMv2Model.from_pretrained('microsoft/layoutlmv2-base-uncased')\n","feature_processor = LayoutLMv2Processor.from_pretrained('microsoft/layoutlmv2-base-uncased')\n","\n","input_feature = feature_processor(doc_image, max_length=512, return_tensors='pt')\n","check_point = time.time()\n","print(layout_torch(**input_feature))\n","print(time.time()-check_point)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1UWSpFfPhi30"},"source":["layout_model = layoutlm.LayoutLM('layoutlmv2-base')\n","layout_model.loading('pytorch_model.bin')\n","\n","input_feature = feature_processor(doc_image, max_length=512, return_tensors='np')\n","check_point = time.time()\n","print(layout_model.propagating(\n","  tf.cast(np.transpose(input_feature['image'], [0, 2, 3, 1]), tf.float32),\n","  input_feature['input_ids'],\n","  input_feature['bbox'],\n","  input_feature['token_type_ids'],\n","  1.-input_feature['attention_mask'])[1])\n","print(time.time()-check_point)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7ViyAJoeYeX7"},"source":["Finetune LayoutLMv2 on FUNSD.\n","*   Need to download the dataset from https://guillaumejaume.github.io/FUNSD.\n","*   Data processing refers to https://github.com/microsoft/unilm/tree/master/layoutlmft.\n","*   Samples longer than max length are truncated for simplicity."]},{"cell_type":"code","metadata":{"id":"H1nkrFu2YeAA"},"source":["!pip install tensorflow-addons\n","!pip install transformers\n","!pip install seqeval\n","!git clone https://github.com/RookieZB/bert_implementation_by_tf2.git\n","!wget https://guillaumejaume.github.io/FUNSD/dataset.zip\n","!wget https://huggingface.co/microsoft/layoutlmv2-base-uncased/resolve/main/pytorch_model.bin\n","!wget https://huggingface.co/microsoft/layoutlmv2-base-uncased/resolve/main/vocab.txt\n","!unzip dataset.zip"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hshonhe-Y66M"},"source":["import os\n","import time\n","import json\n","import numpy as np\n","import tensorflow as tf\n","import tensorflow.keras as keras\n","from PIL import Image\n","from transformers import LayoutLMv2FeatureExtractor\n","from seqeval import metrics\n","from bert_implementation_by_tf2 import mymodels\n","from bert_implementation_by_tf2.modules import layoutlm, adamw"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9wpF3iXZY62s"},"source":["MODEL = 'layoutlmv2-base'\n","CKPT = 'pytorch_model.bin'\n","VOCAB = 'vocab.txt'\n","USECRF = False\n","MAXLEN = 512\n","DROP = 0.1\n","LRATE = 5e-5\n","BATCH = 6\n","EPOCH = 10"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7OZHdbXQUOYJ"},"source":["def bbox_normalizing(bbox, size):\n","  return [\n","    int(1000*bbox[0]/size[1]),\n","    int(1000*bbox[1]/size[0]),\n","    int(1000*bbox[2]/size[1]),\n","    int(1000*bbox[3]/size[0])]\n","\n","\n","def image_loading(path):\n","  i1 = np.array(Image.open(path).convert('RGB'))\n","  return i1, (i1.shape[0], i1.shape[1])\n","\n","\n","def data_loading(path, training=False):\n","  idx2, image2, tok2, box2, tag2, tag3, len2, mask2 = [], [], [], [], [], [], [], []\n","\n","  for file1 in os.listdir(path+'/annotations'):\n","    ann1 = json.load(open(path+'/annotations/'+file1, 'rb'))\n","    image1, size1 = image_loading(path+'/images/'+file1.replace('json', 'png'))\n","    idx1, tok1, tag1, box1 = file1[:-5], [], [], []\n","\n","    for item1 in ann1['form']:\n","      w1, label1 = item1['words'], item1['label']\n","      w1 = [i1 for i1 in w1 if i1['text'].strip() != '']\n","\n","      if len(w1) == 0:\n","        continue\n","      \n","      if label1 == 'other':\n","        for i1 in w1:\n","          t1 = bert_tokenizer.encoding(i1['text'], bos=False, sep=False, pad=False)[0]\n","          tok1, tag1, box1 = tok1+t1, tag1+['O']*len(t1), box1+[bbox_normalizing(i1['box'], size1)]*len(t1)\n","          if np.max(bbox_normalizing(i1['box'], size1)) > 1000:\n","            print(size1)\n","            print(i1['box'])\n","            print('***')\n","      \n","      else:\n","        for r1, i1 in enumerate(w1):\n","          t1 = bert_tokenizer.encoding(i1['text'], bos=False, sep=False, pad=False)[0]\n","          b1 = [('I-' if r1 > 0 else 'B-')+label1.upper()]+['I-'+label1.upper()]*(len(t1)-1)\n","          tok1, tag1, box1 = tok1+t1, tag1+b1, box1+[bbox_normalizing(i1['box'], size1)]*len(t1)\n","\n","    tok1 = [101]+tok1[:MAXLEN-2]+[102]\n","    len1, tok1 = len(tok1), tok1+[0]*(MAXLEN-len(tok1))\n","    box1 = [[0,0,0,0]]+box1[:MAXLEN-2]+[[0,0,0,0]]+[[0,0,0,0]]*(MAXLEN-len1)\n","    tag1 = ['O']+tag1[:MAXLEN-2]+['O']\n","    mask1 = [0]*len1+[1]*(MAXLEN-len1)\n","    image1 = feature_extractor(image1)['pixel_values']\n","    idx2, image2 = idx2+[idx1], image2+[image1]\n","    tok2, box2, mask2, len2 = tok2+[tok1], box2+[box1], mask2+[mask1], len2+[len1]\n","    tag2, tag3 = tag2+[tag1[1:-1]], tag3+[[label_map[a1] for a1 in tag1+['O']*(MAXLEN-len1)]]\n","  \n","  image2 = tf.cast(np.transpose(np.concatenate(image2, 0), [0, 2, 3, 1]), tf.float32)\n","  tok2 = tf.cast(np.array(tok2), tf.int32)\n","  box2 = tf.cast(np.array(box2), tf.int32)\n","  mask2 = tf.cast(np.array(mask2), tf.int32)\n","  tag3 = tf.cast(np.eye(len(label_map.keys()))[tag3], tf.float32)\n","  data1 = tf.data.Dataset.from_tensor_slices((image2, tok2, box2, mask2, tag3))\n","  data1 = data1.shuffle(tok2.shape[0]) if training else data1\n","  return idx2, tag2, data1.batch(BATCH)\n","\n","\n","label_map = {\n","  'O': 0,\n","  'B-HEADER': 1,\n","  'I-HEADER': 2,\n","  'B-QUESTION': 3,\n","  'I-QUESTION': 4,\n","  'B-ANSWER': 5,\n","  'I-ANSWER': 6}\n","feature_extractor = LayoutLMv2FeatureExtractor(apply_ocr=False)\n","bert_tokenizer = mymodels.Tokenizer(True, True, False)\n","bert_tokenizer.loading(VOCAB)\n","train_idx, train_label, train_set = data_loading('dataset/training_data', False)\n","dev_idx, dev_label, dev_set = data_loading('dataset/testing_data', False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EnZknijkbUyn"},"source":["class LayoutSeq(keras.Model):\n","  def __init__(self, model, ckpt, drop, category, crf=False, maxlen=512):\n","    super(LayoutSeq, self).__init__()\n","    self.maxlen, self.layout = maxlen, layoutlm.LayoutLM(model)\n","    self.layout.loading(ckpt)\n","    self.drop = keras.layers.Dropout(drop)\n","    self.dense = keras.layers.Dense(category, None if crf else 'softmax')\n","    self.crf = mymodels.CRF(category) if crf else None\n","\n","  def propagating(self, image, text, bbox, segment=None, mask=None, training=False):\n","    x1 = self.layout.propagating(image, text, bbox, segment, mask, training)[1]\n","    return self.dense(self.drop(x1[:, :self.maxlen, :], training=training))\n","\n","\n","class ModelSeq(object):\n","  def __init__(self, model, label):\n","    self.model = model\n","    self.crf = True if self.model.crf is not None else False\n","    self.map = list(label.keys())\n","\n","  def predicting(self, image, text, bbox, mask):\n","    pred1 = self.model.propagating(image, text, bbox, mask=mask, training=False)\n","    collection1 = []\n","\n","    if self.crf:\n","      for i1 in self.model.crf.decoding(pred1, mask):\n","        collection1.append([self.map[int(j1)] for j1 in i1][1:-1])\n","\n","    else:\n","      for i1, seq1 in enumerate(np.argmax(pred1, 2).tolist()):\n","        mask1 = np.argmax(np.array(mask[i1]).tolist()+[1])\n","        collection1.append([self.map[j1] for j1 in seq1[:mask1]][1:-1])\n","\n","    return collection1\n","\n","\n","@tf.function\n","def step_training(image, text, box, mask, y):\n","  with tf.GradientTape() as tape1:\n","    pred1 = layout_model.propagating(image, text, box, mask=mask, training=True)\n","\n","    if USECRF:\n","      cal1 = layout_model.crf.calculating(y, pred1, mask)\n","      loss1 = tf.reduce_mean(cal1)\n","    else:\n","      mask1 = tf.cast((1-mask), tf.float32)\n","      loss1 = tf.reduce_sum(loss_function(y, pred1)*mask1)/tf.reduce_sum(mask1)\n","  \n","  grad1 = tape1.gradient(loss1, layout_model.trainable_variables)\n","  grad1, _ = tf.clip_by_global_norm(grad1, 1.0)\n","  adam_optimizer.apply_gradients(zip(grad1, layout_model.trainable_variables))\n","  train_loss(loss1)\n","\n","\n","def step_evaluating(data, label):\n","  pred1 = []\n","\n","  for i1, t1, b1, m1, _ in data:\n","    pred1 += ner_model.predicting(i1, t1, b1, m1)\n","\n","  return metrics.f1_score(label, pred1), metrics.classification_report(label, pred1)\n","\n","\n","layout_model = LayoutSeq(MODEL, CKPT, DROP, len(label_map.keys()), USECRF, MAXLEN)\n","ner_model = ModelSeq(layout_model, label_map)\n","adam_optimizer = adamw.AdamW(EPOCH*(len(train_idx)//BATCH+1), LRATE)\n","loss_function = keras.losses.CategoricalCrossentropy(reduction=keras.losses.Reduction.NONE)\n","train_loss = tf.keras.metrics.Mean(name='training_loss')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"K7BwElmTbUsb"},"source":["temp_a = 'Training loss is {:.4f}.'\n","temp_b = 'Dev F1 score is {:.4f}.'\n","batch_count = 0\n","\n","for e_1 in range(EPOCH):\n","  print('Epoch {} running.'.format(e_1+1))\n","\n","  for x_image, x_text, x_box, x_mask, y_tag in train_set:\n","    batch_count = batch_count+1\n","    step_training(x_image, x_text, x_box, x_mask, y_tag)\n","\n","    if batch_count % 5 == 0:\n","      print(temp_a.format(float(train_loss.result())))\n","\n","  f_score, _ = step_evaluating(dev_set, dev_label)\n","  print(temp_b.format(f_score))\n","  print('**********')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1rMNJWdCtxss"},"source":["f_score, c_metrics = step_evaluating(dev_set, dev_label)\n","print(c_metrics)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OnA7q7xIYlEc"},"source":["Finetune LayoutLMv2 on DocVQA dataset, haven't finished.\n","*   Need to download the dataset from https://rrc.cvc.uab.es/.\n","*   Data processing refers to https://github.com/anisha2102/docvqa."]},{"cell_type":"code","metadata":{"id":"savuoJQTYpiB"},"source":["!pip install tensorflow-addons\n","!pip install transformers\n","!git clone https://github.com/RookieZB/bert_implementation_by_tf2.git\n","!wget https://huggingface.co/microsoft/layoutlmv2-base-uncased/resolve/main/pytorch_model.bin\n","!wget https://raw.githubusercontent.com/anisha2102/docvqa/master/create_dataset.py\n","!cp -r drive/MyDrive/Python/Research/tasks/data/doc_vqa ./\n","!tar -xvf doc_vqa/train.tar.gz\n","!tar -xvf doc_vqa/val.tar.gz\n","!tar -xvf doc_vqa/test.tar.gz"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0fMsgHhRYpee"},"source":["import os\n","import time\n","import json\n","import numpy as np\n","import tensorflow as tf\n","import tensorflow.keras as keras\n","from transformers import LayoutLMv2Processor\n","from bert_implementation_by_tf2.modules import layoutlm, adamw"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"b1grIfG3Ypbt"},"source":["MODEL = 'layoutlmv2-base'\n","CKPT = 'pytorch_model.bin'\n","MAXLEN = 512\n","ANSLEN = 30\n","LRATE = 5e-5\n","EPOCH = 2\n","BATCH = 64\n","BEAM = 20"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UfdUABbnYwId"},"source":["class LayoutQA(keras.Model):\n","  def __init__(self, model, ckpt, beam):\n","    super(LayoutQA, self).__init__()\n","    self.layout = layoutlm.LayoutLM(model)\n","    self.layout.loading(ckpt)\n","    self.beam, self.dim = beam, self.layout.param['hidden_size']\n","    self.startdense = keras.layers.Dense(1)\n","    self.mergedense = keras.layers.Dense(512, activation=layoutlm.gelu_activating)\n","    self.enddense = keras.layers.Dense(1)\n","    self.layout.emb.ve = self.layout.emb.add_weight(\n","      'layoutlmv2.visual_segment_embedding', [self.dim], None, keras.initializers.Zeros())\n","        \n","  def propagating(self, image, text, bbox, segment, mask, training=True, start=None):\n","    length1 = mask.shape[1]\n","    seq1 = self.layout.propagating(image, text, bbox, segment, 1.-mask, training)\n","    mask1 = tf.cast(mask*segment, tf.float32)+tf.one_hot(0, length1)\n","    start1 = self.startdense(seq1)[:, :, 0]+1000.0*(mask1-1)\n","    start2 = tf.nn.log_softmax(start1)\n","      \n","    if training:\n","      end0 = seq1\n","      index1 = tf.one_hot(start, depth=length1, axis=-1, dtype=tf.float32)\n","      feat1 = tf.reduce_sum(tf.expand_dims(index1, -1)*seq1, axis=1)\n","      feat1 = tf.tile(tf.expand_dims(feat1, 1), [1, length1, 1])\n","      end1 = tf.concat([feat1, end0], -1)\n","      end1 = self.enddense(self.mergedense(end1))[:, :, 0]\n","      end1 = end1+1000.0*(mask1-1)\n","      end2 = tf.nn.log_softmax(end1)\n","      return start2, end2\n","    else:\n","      prob0, index0 = tf.nn.top_k(start2, k=self.beam)\n","      end0 = tf.tile(tf.expand_dims(seq1, 1), [1, self.beam, 1, 1])\n","      index1 = tf.one_hot(index0, depth=length1, axis=-1, dtype=tf.float32)\n","      feat1 = tf.reduce_sum(tf.expand_dims(seq1, 1)*tf.expand_dims(index1, -1), axis=-2)\n","      feat1 = tf.tile(tf.expand_dims(feat1, 2), [1, 1, length1, 1])\n","      end1 = tf.concat([feat1, end0], -1)\n","      end1 = self.enddense(self.mergedense(end1))[:, :, :, 0]\n","      end1 = end1+tf.expand_dims(1000.0*(mask1-1), 1)\n","      end2 = tf.nn.log_softmax(end1)\n","      prob1, index1 = tf.nn.top_k(end2, k=self.beam)\n","      return start2, end2, prob0, index0, prob1, index1\n","\n","\n","def loss_computing(predstart, predend, start, end):\n","  length1 = predstart.shape[1]\n","  pos1 = tf.one_hot(start, depth=length1, dtype=tf.float32)\n","  loss1 = -tf.reduce_mean(tf.reduce_sum(pos1*predstart, axis=-1))\n","  pos2 = tf.one_hot(end, depth=length1, dtype=tf.float32)\n","  loss2 = -tf.reduce_mean(tf.reduce_sum(pos2*predend, axis=-1))\n","  return (loss1+loss2)/2.0\n","\n","\n","@tf.function\n","def step_training(image, text, bbox, segment, mask, start, end):\n","  with tf.GradientTape() as tape1:\n","    pred1, pred2 = layout_model.propagating(image, text, bbox, segment, mask, True, start)\n","    loss1 = loss_computing(pred1, pred2, start, end)\n","\n","  grad1 = tape1.gradient(loss1, layout_model.trainable_variables)\n","  grad1, _ = tf.clip_by_global_norm(grad1, 1.0)\n","  adam_optimizer.apply_gradients(zip(grad1, layout_model.trainable_variables))\n","  train_loss(loss1)\n","\n","\n","layout_model = LayoutQA(MODEL, CKPT, BEAM)"],"execution_count":null,"outputs":[]}]}