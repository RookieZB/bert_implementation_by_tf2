{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"text_summarization.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"toQJl6Q4xjDp"},"source":["A text summarization example for GPT-2 (not sure this implementation is completely correct).  \n","The data is XSum from https://github.com/EdinburghNLP/XSum."]},{"cell_type":"code","metadata":{"id":"wV-pmJAuQPmG"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dhocVCkxtNX1"},"source":["!pip install nlp\n","!pip install rouge"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rBlT22miZU8c"},"source":["%tensorflow_version 2.x\n","\n","import os\n","import warnings\n","import time\n","import re\n","import nlp\n","import pprint\n","import numpy as np\n","import pandas as pd\n","import tensorflow as tf\n","import tensorflow.keras as keras\n","from rouge import Rouge\n","\n","os.chdir('./drive/My Drive/Python/Research/bert')\n","warnings.filterwarnings('ignore')\n","tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n","\n","import mymodels as mm"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SpjZPQiRF-tG"},"source":["MAXLEN = 256\n","BATCH = 16\n","EPOCH = 2\n","LRATE = 1e-4\n","CONFIG = './models/gpt_base_en/hparams.json'\n","CKPT = './models/gpt_base_en/model.ckpt'\n","VOCAB = './models/gpt_base_en/encoder.json'\n","SAVE = './tasks/models/XSum/model'"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WB2yWLz4u7z5"},"source":["Load and process the data. Use short documents only."]},{"cell_type":"code","metadata":{"id":"zowy4ulbfIK-","executionInfo":{"status":"ok","timestamp":1604244192132,"user_tz":-480,"elapsed":160180,"user":{"displayName":"Chris Chen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gib6pz72VKmdLMqZPhKb2UKgIAW_6HhaEfhtM5C=s64","userId":"15295307384944031263"}},"outputId":"7161fcae-565b-4172-e0c9-363d03d6c5ac","colab":{"base_uri":"https://localhost:8080/","height":212}},"source":["def file_processing(file, droplen):\n","  file['length'] = file.apply(lambda x: len(x['document'].split(' '))+len(x['summary'].split(' ')), axis=1)\n","  return file[file['length'] < droplen*0.9].reset_index(drop=True)\n","\n","\n","def data_processing(data, tokenizer, maxlen, batch, training):\n","  text1, label1, mask1, mlen1 = [], [], [], maxlen+1\n","  pad1 = tokenizer.vocab[tokenizer.pad]\n","\n","  for i1 in range(len(data)):\n","    sum1, doc1 = re.sub('\\n', '', data['summary'][i1]), data['document'][i1]\n","    t1, _, _, l1 = tokenizer.encoding(sum1, None, mlen1, bos=False, pad=False, length=True)\n","    t2, _, _, l2 = tokenizer.encoding(doc1, None, mlen1-l1, bos=False, pad=False, length=True)\n","    t1 = t2+t1+[pad1]*(mlen1-l1-l2)\n","    m1 = [0.]*(l2-1)+[1.]*l1+[0.]*(mlen1-l1-l2)\n","    text1, label1, mask1 = text1+[t1[:-1]], label1+[t1[1:]], mask1+[m1]\n","\n","  text1, label1, mask1 = np.array(text1), np.array(label1), np.array(mask1)\n","  data1 = tf.data.Dataset.from_tensor_slices((text1, label1, mask1))\n","  return data1.shuffle(len(text1)).batch(batch) if training else data1.batch(batch)\n","\n","\n","data_file = nlp.load_dataset('xsum')\n","training_file = file_processing(pd.DataFrame(data_file['train']), MAXLEN)\n","dev_file = file_processing(pd.DataFrame(data_file['validation']), MAXLEN)\n","test_file = file_processing(pd.DataFrame(data_file['test']), MAXLEN)\n","\n","gpt_tokenizer = mm.Tokenizer(False, False, False)\n","gpt_tokenizer.loading(VOCAB)\n","training_set = data_processing(training_file, gpt_tokenizer, MAXLEN, BATCH, False)\n","dev_set = data_processing(dev_file, gpt_tokenizer, MAXLEN, BATCH, False)\n","training_file.head()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Using custom data configuration default\n"],"name":"stderr"},{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>document</th>\n","      <th>summary</th>\n","      <th>length</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>The problem is affecting people using the olde...</td>\n","      <td>Sony has told owners of older models of its Pl...</td>\n","      <td>201</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Only Men Aloud, winners of the BBC's Last Choi...</td>\n","      <td>The winners of a TV talent show have beaten th...</td>\n","      <td>174</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>The National Oceanic and Atmospheric Administr...</td>\n","      <td>As many as 14 hurricanes could hit the Atlanti...</td>\n","      <td>116</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>The 50-year-old musician was treated at a spec...</td>\n","      <td>U2's lead singer Bono has had emergency spinal...</td>\n","      <td>218</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Jordan Romero, from California, telephoned his...</td>\n","      <td>A 13-year-old American boy has become the youn...</td>\n","      <td>210</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                            document  ... length\n","0  The problem is affecting people using the olde...  ...    201\n","1  Only Men Aloud, winners of the BBC's Last Choi...  ...    174\n","2  The National Oceanic and Atmospheric Administr...  ...    116\n","3  The 50-year-old musician was treated at a spec...  ...    218\n","4  Jordan Romero, from California, telephoned his...  ...    210\n","\n","[5 rows x 3 columns]"]},"metadata":{"tags":[]},"execution_count":3}]},{"cell_type":"markdown","metadata":{"id":"MavmUbkNvCVb"},"source":["Load GPT-2 and prepare for training."]},{"cell_type":"code","metadata":{"id":"8GWYVC-w9FEf"},"source":["class GPT2(mm.GPT):\n","  def __init__(self, config):\n","    super(GPT2, self).__init__(config)\n","\n","  @tf.function(experimental_relax_shapes=True)\n","  def iterating(self, x, mask, pos, past, training, softmax):\n","    return super(GPT2, self).propagating(x, mask, pos, past, training, softmax)\n","\n","  def propagating(self, x, mask=None, pos=None, past=None, training=False, softmax=False):\n","    return self.iterating(x, mask, pos, past, training, softmax) if past is not None else super(\n","      GPT2, self).propagating(x, training=training, softmax=softmax)\n","\n","\n","class LM(keras.Model):\n","  def __init__(self, tokenizer, gpt, **kwargs):\n","    super(LM, self).__init__(**kwargs)\n","    self.tokenizer, self.gpt = tokenizer, gpt\n","\n","  def propagating(self, x, training, softmax):\n","    return self.gpt.propagating(x, training=training, softmax=softmax)\n","  \n","  def tokenizing(self, text, maxlen):\n","    text1, mask1, length1 = [], [], []\n","\n","    for i1 in text:\n","      t1, s1, m1, l1 = self.tokenizer.encoding(i1, None, maxlen, bos=False, length=True)\n","      text1, mask1, length1 = text1+[t1], mask1+[m1], length1+[[l1]]\n","\n","    text1, mask1, length1 = np.array(text1), np.array(mask1), np.array(length1)\n","    return text1[:, :int(max(length1))], mask1[:, :int(max(length1))], length1-1\n","\n","  def generating(self, text, beam=5, k=1, p=0.9, temp=1.0, penalty=1.0, orilen=256, genlen=64, minlen=2):\n","    token1, mask1, length1 = self.tokenizing(text, orilen)\n","    gen1, dec1 = self.gpt.generating(token1, mask1, length1, beam, k, p, temp, penalty, genlen, False), []\n","\n","    for i1 in gen1:\n","      curlen1, cur1, out1 = 0, None, None\n","\n","      for j1 in i1:\n","        leng1 = len(j1[:(j1+[self.gpt.eos]).index(self.gpt.eos)])\n","\n","        if leng1 >= minlen:\n","          out1 = self.tokenizer.decoding(j1)\n","          break\n","\n","        if leng1 > curlen1:\n","          curlen1, cur1 = leng1, self.tokenizer.decoding(j1)\n","\n","      dec1 = dec1+[cur1] if out1 is None else dec1+[out1]\n","\n","    return dec1, gen1\n","\n","\n","@tf.function\n","def step_training(x, y, mask):\n","  with tf.GradientTape() as tape1:\n","    pred1 = lm_model.propagating(x, True, True)[0]\n","    mask1 = tf.cast(mask, tf.float32)\n","    loss1 = tf.reduce_sum(loss_function(y, pred1)*mask1)/tf.reduce_sum(mask1)\n","\n","  grad1 = tape1.gradient(loss1, lm_model.trainable_variables)\n","  grad1, _ = tf.clip_by_global_norm(grad1, 1.0)\n","  adam_optimizer.apply_gradients(zip(grad1, lm_model.trainable_variables))\n","  training_loss(loss1)\n","  training_acc(y, pred1, sample_weight=mask1)\n","\n","\n","@tf.function\n","def step_evaluating(x, y, mask):\n","  pred1 = lm_model.propagating(x, False, True)[0]\n","  mask1 = tf.cast(mask, tf.float32)\n","  dev_acc(y, pred1, sample_weight=mask1)\n","\n","\n","gpt_model = GPT2(CONFIG)\n","gpt_model.loading(CKPT)\n","lm_model = LM(gpt_tokenizer, gpt_model)\n","loss_function = keras.losses.SparseCategoricalCrossentropy(reduction=keras.losses.Reduction.NONE)\n","adam_optimizer = mm.AdamWV2(EPOCH*(len(training_file)/BATCH+1), LRATE)\n","\n","training_loss = tf.keras.metrics.Mean(name='training_loss')\n","training_acc = tf.keras.metrics.SparseCategoricalAccuracy(name='training_accuracy')\n","dev_acc = tf.keras.metrics.SparseCategoricalAccuracy(name='dev_accuracy')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RZhMGBEfvGt8"},"source":["Train the model. Here only consider loss and accuracy."]},{"cell_type":"code","metadata":{"id":"5F6LVix7pq0u","executionInfo":{"status":"ok","timestamp":1604250699600,"user_tz":-480,"elapsed":832028,"user":{"displayName":"Chris Chen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gib6pz72VKmdLMqZPhKb2UKgIAW_6HhaEfhtM5C=s64","userId":"15295307384944031263"}},"outputId":"6a3e21cd-84f8-4a68-fc1e-d9be8cf55a2d","colab":{"base_uri":"https://localhost:8080/"}},"source":["log_info = 'Training loss is {:.4f}, and accuracy is {:.4f}.'\n","end_info = 'Dev accuracy is {:.4f}, and epoch cost is {:.4f}.'\n","\n","for epoch_count in range(EPOCH):\n","  print('Epoch {} running.'.format(epoch_count+1))\n","  step_count, time_point = 0, time.time()\n","\n","  for training_x, training_y, training_mask in training_set:\n","    step_training(training_x, training_y, training_mask)\n","    step_count = step_count+1\n","\n","    if step_count % 1000 == 0:\n","      print(log_info.format(float(training_loss.result()), float(training_acc.result())))\n","\n","  for dev_x, dev_y, dev_mask in dev_set:\n","    step_evaluating(dev_x, dev_y, dev_mask)\n","\n","  print(end_info.format(float(dev_acc.result()), time.time()-time_point))\n","  print('**********')\n","  training_acc.reset_states()\n","  dev_acc.reset_states()\n","  lm_model.save_weights(SAVE)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Epoch 1 running.\n","Training loss is 2.5463, and accuracy is 0.4927.\n","Training loss is 2.4302, and accuracy is 0.5081.\n","Training loss is 2.3558, and accuracy is 0.5186.\n","Training loss is 2.3042, and accuracy is 0.5268.\n","Dev accuracy is 0.5553, and epoch cost is 3255.1307.\n","**********\n","Epoch 2 running.\n","Training loss is 2.2051, and accuracy is 0.5870.\n","Training loss is 2.1074, and accuracy is 0.6104.\n","Training loss is 2.0399, and accuracy is 0.6176.\n","Training loss is 1.9933, and accuracy is 0.6202.\n","Dev accuracy is 0.5656, and epoch cost is 3233.4219.\n","**********\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"q3cPISylMrs4"},"source":["Evaluate the model on test set using ROUGE score."]},{"cell_type":"code","metadata":{"id":"S4d6AQ74MqiK","executionInfo":{"status":"ok","timestamp":1604251598235,"user_tz":-480,"elapsed":842622,"user":{"displayName":"Chris Chen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gib6pz72VKmdLMqZPhKb2UKgIAW_6HhaEfhtM5C=s64","userId":"15295307384944031263"}},"outputId":"b4019bf4-0a7d-4795-adb9-8afd85bc6200","colab":{"base_uri":"https://localhost:8080/"}},"source":["lm_model.load_weights(SAVE)\n","test_hyp, test_ref, test_doc = [], test_file['summary'].tolist(), test_file['document'].tolist()\n","\n","for range_id in range(int(np.ceil(len(test_file)/BATCH))):\n","  inp_doc = test_doc[range_id*BATCH:(range_id+1)*BATCH]\n","  test_hyp += lm_model.generating(inp_doc, orilen=MAXLEN, genlen=40, minlen=4)[0]\n","\n","rouge_score = Rouge()\n","rouge_score.get_scores(test_hyp, test_ref, avg=True)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'rouge-1': {'f': 0.3117434702731899,\n","  'p': 0.3769333529610374,\n","  'r': 0.2772046486630661},\n"," 'rouge-2': {'f': 0.12555274489288518,\n","  'p': 0.1520535098089593,\n","  'r': 0.11167880148199814},\n"," 'rouge-l': {'f': 0.2723238256710355,\n","  'p': 0.3291359575144691,\n","  'r': 0.24143634070716324}}"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"markdown","metadata":{"id":"senqVxEUc3ra"},"source":["Check some summarization results from GPT-2."]},{"cell_type":"code","metadata":{"id":"c0XYs5omVyv7","executionInfo":{"status":"ok","timestamp":1604251617750,"user_tz":-480,"elapsed":1960,"user":{"displayName":"Chris Chen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gib6pz72VKmdLMqZPhKb2UKgIAW_6HhaEfhtM5C=s64","userId":"15295307384944031263"}},"outputId":"d6a674fe-3e00-4795-870c-0184d83b4878","colab":{"base_uri":"https://localhost:8080/","height":220}},"source":["sample_doc = test_file['document'][1928]\n","pprint.pprint(sample_doc)\n","lm_model.generating([sample_doc], orilen=MAXLEN, genlen=40, minlen=4)[0][0]"],"execution_count":null,"outputs":[{"output_type":"stream","text":["('The victims were farming in an area that was declared unsafe because of its '\n"," 'close proximity to Mount Sinabung.The volcano was still spewing ash on '\n"," 'Sunday, hampering rescue operations.More than a dozen people were killed '\n"," 'when it erupted in 2014. It also erupted in 2010, after having been dormant '\n"," 'for 400 years.Rescue teams are still scouring the area, looking for more '\n"," 'victims who may have been killed or badly burned by the hot gas and ash '\n"," 'clouds released in the eruption.Rescue teams were searching homes and farms '\n"," 'in the village of Gamber, which was also evacuated in 2014.What causes '\n"," \"volcanoes?The 2,460-metre (8,070 foot) tall volcano is among the country's \"\n"," 'most active.Indonesia, located on the Pacific Ring of Fire, has more than '\n"," '120 active volcanoes.\\n')\n"],"name":"stdout"},{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["' Rescue teams in Indonesia are searching for survivors after a volcano erupted in the capital Jakarta.'"]},"metadata":{"tags":[]},"execution_count":7}]}]}