{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "toQJl6Q4xjDp"
   },
   "source": [
    "A QA example for ELECTRA using Google Colab.  \n",
    "This example is from https://github.com/ymcui/Chinese-ELECTRA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WWnNxbk7Zokr"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lnVYwBuH2OGN"
   },
   "source": [
    "Process data just like source code does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sM7C6KJN2M-N"
   },
   "outputs": [],
   "source": [
    "%tensorflow_version 1.x\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "import json\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "\n",
    "os.chdir('./drive/My Drive/Python/Research/bert')\n",
    "sys.path.append('tasks/datasets/cmrc_2018/utils')\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from tasks.datasets.cmrc_2018.utils import configure_finetuning\n",
    "from tasks.datasets.cmrc_2018.utils.finetune import task_builder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1237,
     "status": "ok",
     "timestamp": 1595755797029,
     "user": {
      "displayName": "Chris Chen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gib6pz72VKmdLMqZPhKb2UKgIAW_6HhaEfhtM5C=s64",
      "userId": "15295307384944031263"
     },
     "user_tz": -480
    },
    "id": "SgZUmoaUsqJT"
   },
   "outputs": [],
   "source": [
    "def data_processing(model, data, param, save, training=True):\n",
    "  with tf.io.gfile.GFile(param, 'r') as file1:\n",
    "    param1 = json.load(file1)\n",
    "\n",
    "  id1, qid1, input1, seg1, mask1, count1 = [], [], [], [], [], 0\n",
    "  start1, end1, token1, map1, max1 = [], [], [], [], []\n",
    "  config1 = configure_finetuning.FinetuningConfig(model, data, **param1)\n",
    "  task1 = task_builder.get_tasks(config1)[0]\n",
    "  data1 = task1.get_examples('train' if training else 'dev')\n",
    "\n",
    "  for example1 in data1:\n",
    "    sample1 = task1.featurize(example1, training, for_eval=True if not training else False)\n",
    "    sample1 = sample1 if isinstance(sample1, list) else [sample1]\n",
    "    count1 = count1+1\n",
    "    \n",
    "    for doc1 in sample1:\n",
    "      qid1.append(example1.qas_id)\n",
    "      id1.append(doc1['cmrc2018_eid'])\n",
    "      input1.append(doc1['input_ids'])\n",
    "      seg1.append(doc1['segment_ids'])\n",
    "      mask1.append(doc1['input_mask'])\n",
    "\n",
    "      if training:\n",
    "        start1.append(doc1['cmrc2018_start_positions'])\n",
    "        end1.append(doc1['cmrc2018_end_positions'])\n",
    "      else:\n",
    "        token1.append(doc1['cmrc2018_tokens'])\n",
    "        map1.append(doc1['cmrc2018_token_to_orig_map'])\n",
    "        max1.append(doc1['cmrc2018_token_is_max_context'])\n",
    "\n",
    "    if count1 % 1000 == 0:\n",
    "      print(str(count1)+' samples processed.')\n",
    "\n",
    "  total1 = {\n",
    "    'id': id1,\n",
    "    'qid': qid1,\n",
    "    'input': input1,\n",
    "    'seg': seg1,\n",
    "    'mask': mask1,\n",
    "    'start': start1,\n",
    "    'end': end1,\n",
    "    'token': token1,\n",
    "    'map': map1,\n",
    "    'max': max1}\n",
    "\n",
    "  with open(save, 'wb') as file1:\n",
    "    pickle.dump(total1, file1)\n",
    "\n",
    "\n",
    "model_1 = 'electra_small_ch'\n",
    "data_1 = 'tasks/datasets/cmrc_2018'\n",
    "param_1 = 'tasks/datasets/cmrc_2018/utils/params_cmrc2018.json'\n",
    "data_processing(model_1, data_1, param_1, 'tasks/datasets/cmrc_2018/train.pkl', True)\n",
    "data_processing(model_1, data_1, param_1, 'tasks/datasets/cmrc_2018/dev.pkl', False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4xpjDy2P228J"
   },
   "source": [
    "Build QA model based on ELECTRA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rBlT22miZU8c"
   },
   "outputs": [],
   "source": [
    "%tensorflow_version 2.x\n",
    "\n",
    "import os\n",
    "import warnings\n",
    "import time\n",
    "import json\n",
    "import pickle\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "\n",
    "os.chdir('./drive/My Drive/Python/Research/bert')\n",
    "warnings.filterwarnings('ignore')\n",
    "nltk.download('punkt')\n",
    "\n",
    "import mymodels as mm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2017,
     "status": "ok",
     "timestamp": 1595755867114,
     "user": {
      "displayName": "Chris Chen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gib6pz72VKmdLMqZPhKb2UKgIAW_6HhaEfhtM5C=s64",
      "userId": "15295307384944031263"
     },
     "user_tz": -480
    },
    "id": "NbYyoo5qaVGU"
   },
   "outputs": [],
   "source": [
    "MODEL = 'electra'\n",
    "VOCAB = 'models/electra_small_ch/vocab.txt'\n",
    "CONFIG = 'models/electra_small_ch/electra_config.json'\n",
    "CKPT = 'models/electra_small_ch/electra_small'\n",
    "SAVE = 'tasks/models/cmrc_2018/model'\n",
    "PRED = 'tasks/datasets/cmrc_2018/pred.json'\n",
    "MAXLEN = 512\n",
    "ANSLEN = 30\n",
    "LRATE = 3e-4\n",
    "BATCH = 32\n",
    "EPOCH = 2\n",
    "BEAM = 20\n",
    "LMODE = 2\n",
    "LDECAY = {\n",
    "  'embedding': 0.8**13,\n",
    "  'encoder/layer_0': 0.8**12,\n",
    "  'encoder/layer_1': 0.8**11,\n",
    "  'encoder/layer_2': 0.8**10,\n",
    "  'encoder/layer_3': 0.8**9,\n",
    "  'encoder/layer_4': 0.8**8,\n",
    "  'encoder/layer_5': 0.8**7,\n",
    "  'encoder/layer_6': 0.8**6,\n",
    "  'encoder/layer_7': 0.8**5,\n",
    "  'encoder/layer_8': 0.8**4,\n",
    "  'encoder/layer_9': 0.8**3,\n",
    "  'encoder/layer_10': 0.8**2,\n",
    "  'encoder/layer_11': 0.8**1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 15254,
     "status": "ok",
     "timestamp": 1595755885632,
     "user": {
      "displayName": "Chris Chen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gib6pz72VKmdLMqZPhKb2UKgIAW_6HhaEfhtM5C=s64",
      "userId": "15295307384944031263"
     },
     "user_tz": -480
    },
    "id": "uS5lZLWeNLju"
   },
   "outputs": [],
   "source": [
    "def data_processing(path, batch, training=True):\n",
    "  with open(path, 'rb') as file1:\n",
    "    data1 = pickle.load(file1)\n",
    "\n",
    "  input1 = np.array(data1['input'])\n",
    "  seg1 = np.array(data1['seg'])\n",
    "  mask1 = np.array(data1['mask'])\n",
    "  len1 = len(data1['input'])\n",
    "\n",
    "  if training:\n",
    "    start1 = np.array(data1['start'])\n",
    "    end1 = np.array(data1['end'])\n",
    "    data2 = tf.data.Dataset.from_tensor_slices((input1, seg1, mask1, start1, end1))\n",
    "    return data2.shuffle(len(start1)).batch(batch), data1, len1\n",
    "  else:\n",
    "    data2 = tf.data.Dataset.from_tensor_slices((input1, seg1, mask1))\n",
    "    return data2.batch(batch), data1, len1\n",
    "\n",
    "\n",
    "training_1, file_1, len_1 = data_processing('tasks/datasets/cmrc_2018/train.pkl', BATCH, True)\n",
    "dev_1, file_2, len_2 = data_processing('tasks/datasets/cmrc_2018/dev.pkl', BATCH, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1312,
     "status": "ok",
     "timestamp": 1595755888876,
     "user": {
      "displayName": "Chris Chen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gib6pz72VKmdLMqZPhKb2UKgIAW_6HhaEfhtM5C=s64",
      "userId": "15295307384944031263"
     },
     "user_tz": -480
    },
    "id": "XemqvX-QEJeh"
   },
   "outputs": [],
   "source": [
    "class ModelELECTRA(keras.Model):\n",
    "  def __init__(self, model, config, beam):\n",
    "    super(ModelELECTRA, self).__init__()\n",
    "    self.beam = beam\n",
    "    self.bert = mm.BERT(config, model, 'seq')\n",
    "    self.dense1 = keras.layers.Dense(1)\n",
    "    self.dense2 = keras.layers.Dense(512, activation=mm.gelu_activating)\n",
    "    self.dense3 = keras.layers.Dense(1)\n",
    "        \n",
    "  def propagating(self, text, segment, mask, training=True, start=None):\n",
    "    length1 = mask.shape[1]\n",
    "    seq1 = self.bert.propagating(text, segment, 1-mask, training)\n",
    "    mask1 = tf.cast(mask*segment, tf.float32)+tf.one_hot(0, length1)\n",
    "    start1 = self.dense1(seq1)[:, :, 0]+1000.0*(mask1-1)\n",
    "    start2 = tf.nn.log_softmax(start1)\n",
    "      \n",
    "    if training:\n",
    "      end0 = seq1\n",
    "      index1 = tf.one_hot(start, depth=length1, axis=-1, dtype=tf.float32)\n",
    "      feat1 = tf.reduce_sum(tf.expand_dims(index1, -1)*seq1, axis=1)\n",
    "      feat1 = tf.tile(tf.expand_dims(feat1, 1), [1, length1, 1])\n",
    "      end1 = tf.concat([feat1, end0], -1)\n",
    "      end1 = self.dense3(self.dense2(end1))[:, :, 0]\n",
    "      end1 = end1+1000.0*(mask1-1)\n",
    "      end2 = tf.nn.log_softmax(end1)\n",
    "      return start2, end2\n",
    "    else:\n",
    "      prob0, index0 = tf.nn.top_k(start2, k=self.beam)\n",
    "      end0 = tf.tile(tf.expand_dims(seq1, 1), [1, self.beam, 1, 1])\n",
    "      index1 = tf.one_hot(index0, depth=length1, axis=-1, dtype=tf.float32)\n",
    "      feat1 = tf.reduce_sum(tf.expand_dims(seq1, 1)*tf.expand_dims(index1, -1), axis=-2)\n",
    "      feat1 = tf.tile(tf.expand_dims(feat1, 2), [1, 1, length1, 1])\n",
    "      end1 = tf.concat([feat1, end0], -1)\n",
    "      end1 = self.dense3(self.dense2(end1))[:, :, :, 0]\n",
    "      end1 = end1+tf.expand_dims(1000.0*(mask1-1), 1)\n",
    "      end2 = tf.nn.log_softmax(end1)\n",
    "      prob1, index1 = tf.nn.top_k(end2, k=self.beam)\n",
    "      return start2, end2, prob0, index0, prob1, index1\n",
    "\n",
    "\n",
    "class ModelQA(object):\n",
    "  def __init__(self, tokenizer, model, maxlen, anslen):\n",
    "    self.maxlen, self.anslen = maxlen, anslen\n",
    "    self.model = model\n",
    "    self.tokenizer = tokenizer\n",
    "    self.vocab = list(self.tokenizer.vocab.keys())\n",
    "\n",
    "  def processing(self, data, label=False):\n",
    "    text0, seg0, mask0, start0, end0 = [], [], [], [], []\n",
    "    \n",
    "    for i1 in data:\n",
    "      text1, segm1, mask1 = self.tokenizer.encoding(i1[0], i1[1], self.maxlen)\n",
    "      text0.append(text1)\n",
    "      seg0.append(segm1)\n",
    "      mask0.append(mask1)\n",
    "      \n",
    "      if label:\n",
    "        start0.append(i1[2])\n",
    "        end0.append(i1[3])\n",
    "\n",
    "    text0 = np.array(text0)\n",
    "    seg0 = np.array(seg0)\n",
    "    mask0 = 1-np.array(mask0)\n",
    "    start0 = np.array(start0)\n",
    "    end0 = np.array(end0)\n",
    "    return text0, seg0, mask0, start0, end0\n",
    "\n",
    "  def searching(self, input, seg, mask, constraint=None):\n",
    "    pred1, pred2, prob1, index1, prob2, index2 = self.model.propagating(input, seg, mask, False)\n",
    "    prob1, index1, prob2, index2 = prob1.numpy(), index1.numpy(), prob2.numpy(), index2.numpy()\n",
    "    reply0 = []\n",
    "\n",
    "    if constraint is not None:\n",
    "      token1, map1, max1 = constraint[0], constraint[1], constraint[2]\n",
    "\n",
    "    for b1 in range(index1.shape[0]):\n",
    "      reply1, value1 = 'empty', -1000\n",
    "\n",
    "      for s1 in range(index1.shape[1]):\n",
    "        for e1 in range(index2.shape[2]):\n",
    "          start1, sprob1 = index1[b1, s1], prob1[b1, s1]\n",
    "          end1, eprob1 = index2[b1, s1, e1], prob2[b1, s1, e1]\n",
    "\n",
    "          if sprob1+eprob1 < value1:\n",
    "            continue\n",
    "          if start1 == 0:\n",
    "            continue\n",
    "          if start1 > end1:\n",
    "            continue\n",
    "          if end1-start1+1 > self.anslen:\n",
    "            continue\n",
    "\n",
    "          if constraint is not None:\n",
    "            if start1 >= len(token1[b1]):\n",
    "              continue\n",
    "            if end1 >= len(token1[b1]):\n",
    "              continue\n",
    "            if start1 not in map1[b1]:\n",
    "              continue\n",
    "            if end1 not in map1[b1]:\n",
    "              continue\n",
    "            if not max1[b1].get(start1, False):\n",
    "              continue\n",
    "\n",
    "          if constraint is not None:\n",
    "            reply1, value1 = ''.join(token1[b1][start1:end1+1]), sprob1+eprob1\n",
    "          else:\n",
    "            list1 = [self.vocab[i1] for i1 in input[b1][start1:end1+1]]\n",
    "            reply1, value1 = ''.join(list1), sprob1+eprob1\n",
    "\n",
    "      reply1 = reply1.replace(' ##', '').replace('##', '')\n",
    "      reply0.append({'reply': reply1, 'probability': value1})\n",
    "      # To do: too lazy to make the final process of predicted answers.\n",
    "      \n",
    "    return reply0\n",
    "\n",
    "  def replying(self, context, question):\n",
    "    text1, seg1, mask1, _, _ = self.processing([[question, context]])\n",
    "    return self.searching(text1, seg1, mask1)[0]['reply']\n",
    "\n",
    "\n",
    "def loss_computing(predstart, predend, start, end):\n",
    "  length1 = predstart.shape[1]\n",
    "  pos1 = tf.one_hot(start, depth=length1, dtype=tf.float32)\n",
    "  loss1 = -tf.reduce_mean(tf.reduce_sum(pos1*predstart, axis=-1))\n",
    "  pos2 = tf.one_hot(end, depth=length1, dtype=tf.float32)\n",
    "  loss2 = -tf.reduce_mean(tf.reduce_sum(pos2*predend, axis=-1))\n",
    "  return (loss1+loss2)/2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 138782,
     "status": "ok",
     "timestamp": 1595756033328,
     "user": {
      "displayName": "Chris Chen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gib6pz72VKmdLMqZPhKb2UKgIAW_6HhaEfhtM5C=s64",
      "userId": "15295307384944031263"
     },
     "user_tz": -480
    },
    "id": "JzGcbHZyEJbN"
   },
   "outputs": [],
   "source": [
    "tokenizer_1 = mm.Tokenizer()\n",
    "tokenizer_1.loading(VOCAB)\n",
    "model_1 = ModelELECTRA(MODEL, CONFIG, BEAM)\n",
    "model_1.bert.loading(CKPT)\n",
    "optimizer_1 = mm.AdamW(EPOCH*(int(len_1/BATCH)+1), LRATE, lmode=LMODE, ldecay=LDECAY)\n",
    "loss_1 = tf.keras.metrics.Mean(name='training_loss')\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def step_training(text, segment, mask, start, end):\n",
    "  with tf.GradientTape() as tape_1:\n",
    "    pred_1, pred_2 = model_1.propagating(text, segment, mask, True, start)\n",
    "    loss_0 = loss_computing(pred_1, pred_2, start, end)\n",
    "\n",
    "  grad_1 = tape_1.gradient(loss_0, model_1.trainable_variables)\n",
    "  grad_1, _ = tf.clip_by_global_norm(grad_1, 1.0)\n",
    "  optimizer_1.apply_gradients(zip(grad_1, model_1.trainable_variables))\n",
    "  loss_1(loss_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 185
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 596153,
     "status": "ok",
     "timestamp": 1595756660512,
     "user": {
      "displayName": "Chris Chen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gib6pz72VKmdLMqZPhKb2UKgIAW_6HhaEfhtM5C=s64",
      "userId": "15295307384944031263"
     },
     "user_tz": -480
    },
    "id": "c_ugPwSkEJZD",
    "outputId": "a3ef68fe-e054-418e-81fc-bf4629ef0206"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 running, training loss is 3.292, and step cost is 0.5161.\n",
      "Epoch 1 running, training loss is 1.9053, and step cost is 0.5154.\n",
      "Epoch 1 running, training loss is 1.6242, and step cost is 0.5158.\n",
      "Epoch 1 running, training loss is 1.4967, and step cost is 0.5157.\n",
      "Epoch 1 running, training loss is 1.3801, and step cost is 0.5158.\n",
      "Epoch 2 running, training loss is 1.2286, and step cost is 0.5191.\n",
      "Epoch 2 running, training loss is 1.1254, and step cost is 0.5184.\n",
      "Epoch 2 running, training loss is 1.1072, and step cost is 0.514.\n",
      "Epoch 2 running, training loss is 1.0529, and step cost is 0.5176.\n",
      "Epoch 2 running, training loss is 1.0585, and step cost is 0.5173.\n"
     ]
    }
   ],
   "source": [
    "temp_1 = 'Epoch {} running, training loss is {}, and step cost is {}.'\n",
    "count_1 = 0\n",
    "\n",
    "for e_1 in range(EPOCH):\n",
    "  for x_1, x_2, x_3, y_1, y_2 in training_1:\n",
    "    time_1, count_1 = time.time(), count_1+1\n",
    "    step_training(x_1, x_2, x_3, y_1, y_2)\n",
    "\n",
    "    if count_1 % 100 == 0:\n",
    "      o_1 = round(float(loss_1.result()), 4)\n",
    "      print(temp_1.format(e_1+1, o_1, round(time.time()-time_1, 4)))\n",
    "      loss_1.reset_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2705,
     "status": "ok",
     "timestamp": 1595756681673,
     "user": {
      "displayName": "Chris Chen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gib6pz72VKmdLMqZPhKb2UKgIAW_6HhaEfhtM5C=s64",
      "userId": "15295307384944031263"
     },
     "user_tz": -480
    },
    "id": "uJC1QoWsjotQ",
    "outputId": "e672c873-9492-45bc-d58f-4d3e08ae2b9e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "上海浦东新区人氏\n"
     ]
    }
   ],
   "source": [
    "model_1.save_weights(SAVE)\n",
    "qamodel_1 = ModelQA(tokenizer_1, model_1, MAXLEN, ANSLEN)\n",
    "qamodel_1.model = model_1\n",
    "\n",
    "context_1 = '陈某生于一九九三年，上海浦东新区人氏。他英俊潇洒、骁勇善战、足智多谋，是不可多得的猛将。'\n",
    "question_1 = '陈某是哪里人？'\n",
    "print(qamodel_1.replying(context_1, question_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 47964,
     "status": "ok",
     "timestamp": 1595756732821,
     "user": {
      "displayName": "Chris Chen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gib6pz72VKmdLMqZPhKb2UKgIAW_6HhaEfhtM5C=s64",
      "userId": "15295307384944031263"
     },
     "user_tz": -480
    },
    "id": "jrPPwTINyV3g"
   },
   "outputs": [],
   "source": [
    "df_1 = pd.DataFrame()\n",
    "\n",
    "for i_1, x_1 in enumerate(dev_1):\n",
    "  id_1 = file_2['id'][i_1*BATCH:(i_1+1)*BATCH]\n",
    "  qid_1 = file_2['qid'][i_1*BATCH:(i_1+1)*BATCH]\n",
    "  tok_1 = file_2['token'][i_1*BATCH:(i_1+1)*BATCH]\n",
    "  map_1 = file_2['map'][i_1*BATCH:(i_1+1)*BATCH]\n",
    "  max_1 = file_2['max'][i_1*BATCH:(i_1+1)*BATCH]\n",
    "\n",
    "  reply_1 = pd.DataFrame(qamodel_1.searching(x_1[0], x_1[1], x_1[2], [tok_1, map_1, max_1]))\n",
    "  reply_1['id'] = id_1\n",
    "  reply_1['qid'] = qid_1\n",
    "  df_1 = df_1.append(reply_1, ignore_index=True)\n",
    "\n",
    "df_1 = df_1.sort_values(['probability']).reset_index(drop=True).drop_duplicates(['qid'], keep='last')\n",
    "df_1 = df_1.sort_values(['id']).reset_index(drop=True)\n",
    "json.dump(df_1.set_index('qid')['reply'].to_dict(), open(PRED, 'w'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HSreORgS90aW"
   },
   "source": [
    "Evaluate QA model by provided script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6802,
     "status": "ok",
     "timestamp": 1595756745560,
     "user": {
      "displayName": "Chris Chen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gib6pz72VKmdLMqZPhKb2UKgIAW_6HhaEfhtM5C=s64",
      "userId": "15295307384944031263"
     },
     "user_tz": -480
    },
    "id": "I59QunQJ9zaM",
    "outputId": "cbb4305d-fd17-48ad-a33c-0ffe8185ea0c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"AVERAGE\": \"71.159\", \"F1\": \"80.528\", \"EM\": \"61.789\", \"TOTAL\": 3219, \"SKIP\": 0}\n"
     ]
    }
   ],
   "source": [
    "!python \\\n",
    "  tasks/datasets/cmrc_2018/utils/cmrc2018_drcd_evaluate.py \\\n",
    "  tasks/datasets/cmrc_2018/dev.json \\\n",
    "  tasks/datasets/cmrc_2018/pred.json"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "text_qa.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
