{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"text_qa.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"toQJl6Q4xjDp","colab_type":"text"},"source":["A QA example for ELECTRA.  \n","This example is from https://github.com/ymcui/Chinese-ELECTRA."]},{"cell_type":"code","metadata":{"id":"WWnNxbk7Zokr","colab_type":"code","colab":{}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lnVYwBuH2OGN","colab_type":"text"},"source":["Process data just like source code does."]},{"cell_type":"code","metadata":{"id":"sM7C6KJN2M-N","colab_type":"code","colab":{}},"source":["%tensorflow_version 1.x\n","\n","import os\n","import sys\n","import warnings\n","import json\n","import pickle\n","import tensorflow as tf\n","\n","os.chdir('./drive/My Drive/Python/Research/bert')\n","sys.path.append('tasks/datasets/cmrc_2018/utils')\n","warnings.filterwarnings('ignore')\n","\n","from tasks.datasets.cmrc_2018.utils import configure_finetuning\n","from tasks.datasets.cmrc_2018.utils.finetune import task_builder"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SgZUmoaUsqJT","colab_type":"code","colab":{}},"source":["def data_processing(model, data, param, save, training=True):\n","  with tf.io.gfile.GFile(param, 'r') as file1:\n","    param1 = json.load(file1)\n","\n","  id1, qid1, input1, seg1, mask1, count1 = [], [], [], [], [], 0\n","  start1, end1, token1, map1, max1 = [], [], [], [], []\n","  config1 = configure_finetuning.FinetuningConfig(model, data, **param1)\n","  task1 = task_builder.get_tasks(config1)[0]\n","  data1 = task1.get_examples('train' if training else 'dev')\n","\n","  for example1 in data1:\n","    sample1 = task1.featurize(example1, training, for_eval=True if not training else False)\n","    sample1 = sample1 if isinstance(sample1, list) else [sample1]\n","    count1 = count1+1\n","    \n","    for doc1 in sample1:\n","      qid1.append(example1.qas_id)\n","      id1.append(doc1['cmrc2018_eid'])\n","      input1.append(doc1['input_ids'])\n","      seg1.append(doc1['segment_ids'])\n","      mask1.append(doc1['input_mask'])\n","\n","      if training:\n","        start1.append(doc1['cmrc2018_start_positions'])\n","        end1.append(doc1['cmrc2018_end_positions'])\n","      else:\n","        token1.append(doc1['cmrc2018_tokens'])\n","        map1.append(doc1['cmrc2018_token_to_orig_map'])\n","        max1.append(doc1['cmrc2018_token_is_max_context'])\n","\n","    if count1 % 1000 == 0:\n","      print(str(count1)+' samples processed.')\n","\n","  total1 = {\n","    'id': id1,\n","    'qid': qid1,\n","    'input': input1,\n","    'seg': seg1,\n","    'mask': mask1,\n","    'start': start1,\n","    'end': end1,\n","    'token': token1,\n","    'map': map1,\n","    'max': max1}\n","\n","  with open(save, 'wb') as file1:\n","    pickle.dump(total1, file1)\n","\n","\n","model_1 = 'electra_small_ch'\n","data_1 = 'tasks/datasets/cmrc_2018'\n","param_1 = 'tasks/datasets/cmrc_2018/utils/params_cmrc2018.json'\n","data_processing(model_1, data_1, param_1, 'tasks/datasets/cmrc_2018/train.pkl', True)\n","data_processing(model_1, data_1, param_1, 'tasks/datasets/cmrc_2018/dev.pkl', False)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4xpjDy2P228J","colab_type":"text"},"source":["Build QA model based on ELECTRA."]},{"cell_type":"code","metadata":{"id":"rBlT22miZU8c","colab_type":"code","colab":{}},"source":["%tensorflow_version 2.x\n","\n","import os\n","import warnings\n","import time\n","import json\n","import pickle\n","import nltk\n","import numpy as np\n","import pandas as pd\n","import tensorflow as tf\n","import tensorflow.keras as keras\n","\n","os.chdir('./drive/My Drive/Python/Research/bert')\n","warnings.filterwarnings('ignore')\n","nltk.download('punkt')\n","\n","import mymodels as mm"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NbYyoo5qaVGU","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1596322728952,"user_tz":-480,"elapsed":944,"user":{"displayName":"Chris Chen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gib6pz72VKmdLMqZPhKb2UKgIAW_6HhaEfhtM5C=s64","userId":"15295307384944031263"}}},"source":["MODEL = 'electra'\n","VOCAB = 'models/electra_small_ch/vocab.txt'\n","CONFIG = 'models/electra_small_ch/electra_config.json'\n","CKPT = 'models/electra_small_ch/electra_small'\n","SAVE = 'tasks/models/cmrc_2018/model'\n","PRED = 'tasks/datasets/cmrc_2018/pred.json'\n","MAXLEN = 512\n","ANSLEN = 30\n","LRATE = 3e-4\n","BATCH = 32\n","EPOCH = 2\n","BEAM = 20\n","LMODE = 2\n","LDECAY = {\n","  'embedding': 0.8**13,\n","  'encoder/layer_0': 0.8**12,\n","  'encoder/layer_1': 0.8**11,\n","  'encoder/layer_2': 0.8**10,\n","  'encoder/layer_3': 0.8**9,\n","  'encoder/layer_4': 0.8**8,\n","  'encoder/layer_5': 0.8**7,\n","  'encoder/layer_6': 0.8**6,\n","  'encoder/layer_7': 0.8**5,\n","  'encoder/layer_8': 0.8**4,\n","  'encoder/layer_9': 0.8**3,\n","  'encoder/layer_10': 0.8**2,\n","  'encoder/layer_11': 0.8**1}"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"uS5lZLWeNLju","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1596322736163,"user_tz":-480,"elapsed":5923,"user":{"displayName":"Chris Chen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gib6pz72VKmdLMqZPhKb2UKgIAW_6HhaEfhtM5C=s64","userId":"15295307384944031263"}}},"source":["def data_processing(path, batch, training=True):\n","  with open(path, 'rb') as file1:\n","    data1 = pickle.load(file1)\n","\n","  input1 = np.array(data1['input'])\n","  seg1 = np.array(data1['seg'])\n","  mask1 = np.array(data1['mask'])\n","  len1 = len(data1['input'])\n","\n","  if training:\n","    start1 = np.array(data1['start'])\n","    end1 = np.array(data1['end'])\n","    data2 = tf.data.Dataset.from_tensor_slices((input1, seg1, mask1, start1, end1))\n","    return data2.shuffle(len(start1)).batch(batch), data1, len1\n","  else:\n","    data2 = tf.data.Dataset.from_tensor_slices((input1, seg1, mask1))\n","    return data2.batch(batch), data1, len1\n","\n","\n","training_1, file_1, len_1 = data_processing('tasks/datasets/cmrc_2018/train.pkl', BATCH, True)\n","dev_1, file_2, len_2 = data_processing('tasks/datasets/cmrc_2018/dev.pkl', BATCH, False)"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"XemqvX-QEJeh","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1596322739809,"user_tz":-480,"elapsed":937,"user":{"displayName":"Chris Chen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gib6pz72VKmdLMqZPhKb2UKgIAW_6HhaEfhtM5C=s64","userId":"15295307384944031263"}}},"source":["class ModelELECTRA(keras.Model):\n","  def __init__(self, model, config, beam):\n","    super(ModelELECTRA, self).__init__()\n","    self.beam = beam\n","    self.bert = mm.BERT(config, model, 'seq')\n","    self.dense1 = keras.layers.Dense(1)\n","    self.dense2 = keras.layers.Dense(512, activation=mm.gelu_activating)\n","    self.dense3 = keras.layers.Dense(1)\n","        \n","  def propagating(self, text, segment, mask, training=True, start=None):\n","    length1 = mask.shape[1]\n","    seq1 = self.bert.propagating(text, segment, 1-mask, training)\n","    mask1 = tf.cast(mask*segment, tf.float32)+tf.one_hot(0, length1)\n","    start1 = self.dense1(seq1)[:, :, 0]+1000.0*(mask1-1)\n","    start2 = tf.nn.log_softmax(start1)\n","      \n","    if training:\n","      end0 = seq1\n","      index1 = tf.one_hot(start, depth=length1, axis=-1, dtype=tf.float32)\n","      feat1 = tf.reduce_sum(tf.expand_dims(index1, -1)*seq1, axis=1)\n","      feat1 = tf.tile(tf.expand_dims(feat1, 1), [1, length1, 1])\n","      end1 = tf.concat([feat1, end0], -1)\n","      end1 = self.dense3(self.dense2(end1))[:, :, 0]\n","      end1 = end1+1000.0*(mask1-1)\n","      end2 = tf.nn.log_softmax(end1)\n","      return start2, end2\n","    else:\n","      prob0, index0 = tf.nn.top_k(start2, k=self.beam)\n","      end0 = tf.tile(tf.expand_dims(seq1, 1), [1, self.beam, 1, 1])\n","      index1 = tf.one_hot(index0, depth=length1, axis=-1, dtype=tf.float32)\n","      feat1 = tf.reduce_sum(tf.expand_dims(seq1, 1)*tf.expand_dims(index1, -1), axis=-2)\n","      feat1 = tf.tile(tf.expand_dims(feat1, 2), [1, 1, length1, 1])\n","      end1 = tf.concat([feat1, end0], -1)\n","      end1 = self.dense3(self.dense2(end1))[:, :, :, 0]\n","      end1 = end1+tf.expand_dims(1000.0*(mask1-1), 1)\n","      end2 = tf.nn.log_softmax(end1)\n","      prob1, index1 = tf.nn.top_k(end2, k=self.beam)\n","      return start2, end2, prob0, index0, prob1, index1\n","\n","\n","class ModelQA(object):\n","  def __init__(self, tokenizer, model, maxlen, anslen):\n","    self.maxlen, self.anslen = maxlen, anslen\n","    self.model = model\n","    self.tokenizer = tokenizer\n","    self.vocab = list(self.tokenizer.vocab.keys())\n","\n","  def processing(self, data, label=False):\n","    text0, seg0, mask0, start0, end0 = [], [], [], [], []\n","    \n","    for i1 in data:\n","      text1, segm1, mask1 = self.tokenizer.encoding(i1[0], i1[1], self.maxlen)\n","      text0.append(text1)\n","      seg0.append(segm1)\n","      mask0.append(mask1)\n","      \n","      if label:\n","        start0.append(i1[2])\n","        end0.append(i1[3])\n","\n","    text0 = np.array(text0)\n","    seg0 = np.array(seg0)\n","    mask0 = 1-np.array(mask0)\n","    start0 = np.array(start0)\n","    end0 = np.array(end0)\n","    return text0, seg0, mask0, start0, end0\n","\n","  def searching(self, input, seg, mask, constraint=None):\n","    pred1, pred2, prob1, index1, prob2, index2 = self.model.propagating(input, seg, mask, False)\n","    prob1, index1, prob2, index2 = prob1.numpy(), index1.numpy(), prob2.numpy(), index2.numpy()\n","    reply0 = []\n","\n","    if constraint is not None:\n","      token1, map1, max1 = constraint[0], constraint[1], constraint[2]\n","\n","    for b1 in range(index1.shape[0]):\n","      reply1, value1 = 'empty', -1000\n","\n","      for s1 in range(index1.shape[1]):\n","        for e1 in range(index2.shape[2]):\n","          start1, sprob1 = index1[b1, s1], prob1[b1, s1]\n","          end1, eprob1 = index2[b1, s1, e1], prob2[b1, s1, e1]\n","\n","          if sprob1+eprob1 < value1:\n","            continue\n","          if start1 == 0:\n","            continue\n","          if start1 > end1:\n","            continue\n","          if end1-start1+1 > self.anslen:\n","            continue\n","\n","          if constraint is not None:\n","            if start1 >= len(token1[b1]):\n","              continue\n","            if end1 >= len(token1[b1]):\n","              continue\n","            if start1 not in map1[b1]:\n","              continue\n","            if end1 not in map1[b1]:\n","              continue\n","            if not max1[b1].get(start1, False):\n","              continue\n","\n","          if constraint is not None:\n","            reply1, value1 = ''.join(token1[b1][start1:end1+1]), sprob1+eprob1\n","          else:\n","            list1 = [self.vocab[i1] for i1 in input[b1][start1:end1+1]]\n","            reply1, value1 = ''.join(list1), sprob1+eprob1\n","\n","      reply1 = reply1.replace(' ##', '').replace('##', '')\n","      reply0.append({'reply': reply1, 'probability': value1})\n","      # To do: too lazy to make the final process of predicted answers.\n","      \n","    return reply0\n","\n","  def replying(self, context, question):\n","    text1, seg1, mask1, _, _ = self.processing([[question, context]])\n","    return self.searching(text1, seg1, mask1)[0]['reply']\n","\n","\n","def loss_computing(predstart, predend, start, end):\n","  length1 = predstart.shape[1]\n","  pos1 = tf.one_hot(start, depth=length1, dtype=tf.float32)\n","  loss1 = -tf.reduce_mean(tf.reduce_sum(pos1*predstart, axis=-1))\n","  pos2 = tf.one_hot(end, depth=length1, dtype=tf.float32)\n","  loss2 = -tf.reduce_mean(tf.reduce_sum(pos2*predend, axis=-1))\n","  return (loss1+loss2)/2.0"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"JzGcbHZyEJbN","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1596322760088,"user_tz":-480,"elapsed":17983,"user":{"displayName":"Chris Chen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gib6pz72VKmdLMqZPhKb2UKgIAW_6HhaEfhtM5C=s64","userId":"15295307384944031263"}}},"source":["tokenizer_1 = mm.Tokenizer()\n","tokenizer_1.loading(VOCAB)\n","model_1 = ModelELECTRA(MODEL, CONFIG, BEAM)\n","model_1.bert.loading(CKPT)\n","optimizer_1 = mm.AdamW(EPOCH*(int(len_1/BATCH)+1), LRATE, lmode=LMODE, ldecay=LDECAY)\n","loss_1 = tf.keras.metrics.Mean(name='training_loss')\n","\n","\n","@tf.function\n","def step_training(text, segment, mask, start, end):\n","  with tf.GradientTape() as tape_1:\n","    pred_1, pred_2 = model_1.propagating(text, segment, mask, True, start)\n","    loss_0 = loss_computing(pred_1, pred_2, start, end)\n","\n","  grad_1 = tape_1.gradient(loss_0, model_1.trainable_variables)\n","  grad_1, _ = tf.clip_by_global_norm(grad_1, 1.0)\n","  optimizer_1.apply_gradients(zip(grad_1, model_1.trainable_variables))\n","  loss_1(loss_0)"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"c_ugPwSkEJZD","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":286},"executionInfo":{"status":"ok","timestamp":1596323319640,"user_tz":-480,"elapsed":554907,"user":{"displayName":"Chris Chen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gib6pz72VKmdLMqZPhKb2UKgIAW_6HhaEfhtM5C=s64","userId":"15295307384944031263"}},"outputId":"51fc8bee-645d-40f8-f53c-6e9ed8a0572b"},"source":["temp_1 = 'Training loss is {}, and step cost is {}.'\n","temp_2 = 'Epoch cost is {}.'\n","count_1 = 0\n","\n","for e_1 in range(EPOCH):\n","  print('Epoch {} running.'.format(e_1+1))\n","  time_0 = time.time()\n","\n","  for x_1, x_2, x_3, y_1, y_2 in training_1:\n","    time_1, count_1 = time.time(), count_1+1\n","    step_training(x_1, x_2, x_3, y_1, y_2)\n","\n","    if count_1 % 100 == 0:\n","      o_1 = round(float(loss_1.result()), 4)\n","      print(temp_1.format(o_1, round(time.time()-time_1, 4)))\n","      loss_1.reset_states()\n","\n","  print(temp_2.format(round(time.time()-time_0, 4)))\n","  print('**********')"],"execution_count":6,"outputs":[{"output_type":"stream","text":["Epoch 1 running.\n","Training loss is 3.2278, and step cost is 0.4914.\n","Training loss is 1.8203, and step cost is 0.4927.\n","Training loss is 1.695, and step cost is 0.4917.\n","Training loss is 1.4986, and step cost is 0.4899.\n","Training loss is 1.3778, and step cost is 0.4918.\n","Epoch cost is 291.6065.\n","**********\n","Epoch 2 running.\n","Training loss is 1.2273, and step cost is 0.4917.\n","Training loss is 1.1316, and step cost is 0.4922.\n","Training loss is 1.1354, and step cost is 0.4918.\n","Training loss is 1.0996, and step cost is 0.4906.\n","Training loss is 1.0558, and step cost is 0.492.\n","Epoch cost is 262.3915.\n","**********\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"uJC1QoWsjotQ","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1596323333743,"user_tz":-480,"elapsed":925,"user":{"displayName":"Chris Chen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gib6pz72VKmdLMqZPhKb2UKgIAW_6HhaEfhtM5C=s64","userId":"15295307384944031263"}},"outputId":"4ed1bae8-845b-439b-9273-5b02f083be30"},"source":["model_1.save_weights(SAVE)\n","qamodel_1 = ModelQA(tokenizer_1, model_1, MAXLEN, ANSLEN)\n","\n","context_1 = '陈某生于一九九三年，上海浦东新区人氏。他英俊潇洒、骁勇善战、足智多谋，是不可多得的猛将。'\n","question_1 = '陈某是哪里人？'\n","print(qamodel_1.replying(context_1, question_1))"],"execution_count":7,"outputs":[{"output_type":"stream","text":["上海浦东新区人氏\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"jrPPwTINyV3g","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1596323378501,"user_tz":-480,"elapsed":42124,"user":{"displayName":"Chris Chen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gib6pz72VKmdLMqZPhKb2UKgIAW_6HhaEfhtM5C=s64","userId":"15295307384944031263"}}},"source":["df_1 = pd.DataFrame()\n","\n","for i_1, x_1 in enumerate(dev_1):\n","  id_1 = file_2['id'][i_1*BATCH:(i_1+1)*BATCH]\n","  qid_1 = file_2['qid'][i_1*BATCH:(i_1+1)*BATCH]\n","  tok_1 = file_2['token'][i_1*BATCH:(i_1+1)*BATCH]\n","  map_1 = file_2['map'][i_1*BATCH:(i_1+1)*BATCH]\n","  max_1 = file_2['max'][i_1*BATCH:(i_1+1)*BATCH]\n","\n","  reply_1 = pd.DataFrame(qamodel_1.searching(x_1[0], x_1[1], x_1[2], [tok_1, map_1, max_1]))\n","  reply_1['id'] = id_1\n","  reply_1['qid'] = qid_1\n","  df_1 = df_1.append(reply_1, ignore_index=True)\n","\n","df_1 = df_1.sort_values(['probability']).reset_index(drop=True).drop_duplicates(['qid'], keep='last')\n","df_1 = df_1.sort_values(['id']).reset_index(drop=True)\n","json.dump(df_1.set_index('qid')['reply'].to_dict(), open(PRED, 'w'))"],"execution_count":8,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HSreORgS90aW","colab_type":"text"},"source":["Evaluate QA model by provided script."]},{"cell_type":"code","metadata":{"id":"I59QunQJ9zaM","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1596323391585,"user_tz":-480,"elapsed":4034,"user":{"displayName":"Chris Chen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gib6pz72VKmdLMqZPhKb2UKgIAW_6HhaEfhtM5C=s64","userId":"15295307384944031263"}},"outputId":"fbc10ef6-a221-4d97-e058-6cca46715b09"},"source":["!python \\\n","  tasks/datasets/cmrc_2018/utils/cmrc2018_drcd_evaluate.py \\\n","  tasks/datasets/cmrc_2018/dev.json \\\n","  tasks/datasets/cmrc_2018/pred.json"],"execution_count":9,"outputs":[{"output_type":"stream","text":["{\"AVERAGE\": \"71.196\", \"F1\": \"80.509\", \"EM\": \"61.883\", \"TOTAL\": 3219, \"SKIP\": 0}\n"],"name":"stdout"}]}]}