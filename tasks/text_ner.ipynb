{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"text_ner.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"toQJl6Q4xjDp","colab_type":"text"},"source":["NER example for RoBERTa.  \n","The data is from People's Daily."]},{"cell_type":"code","metadata":{"id":"WWnNxbk7Zokr","colab_type":"code","colab":{}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rBlT22miZU8c","colab_type":"code","colab":{}},"source":["%tensorflow_version 2.x\n","\n","import os\n","import warnings\n","import time\n","import numpy as np\n","import tensorflow as tf\n","import tensorflow.keras as keras\n","from seqeval import metrics\n","\n","os.chdir('./drive/My Drive/Python/Research/bert')\n","warnings.filterwarnings('ignore')\n","\n","import mymodels as mm"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NbYyoo5qaVGU","colab_type":"code","colab":{}},"source":["MODEL = 'roberta'\n","VOCAB = 'models/roberta_base_ch/vocab.txt'\n","CONFIG = 'models/roberta_base_ch/bert_config.json'\n","CKPT = 'models/roberta_base_ch/bert_model.ckpt'\n","USECRF = True\n","MAXLEN = 128\n","DROP = 0.5\n","BATCH = 32\n","EPOCH = 10\n","LRATE = 2e-5\n","DRATE = 1e-2\n","LABEL = {\n","  'O': 0,\n","  'B-PER': 1,\n","  'I-PER': 2,\n","  'B-ORG': 3,\n","  'I-ORG': 4,\n","  'B-LOC': 5,\n","  'I-LOC': 6}"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SbKrd0hmiWLm","colab_type":"code","colab":{}},"source":["def data_processing(file, tokenizer, label, maxlen, batch, training):\n","  file1 = open(file, encoding='utf-8').read().split('\\n\\n')\n","  file1 = [f1 for f1 in file1 if len(f1) > 0]\n","  token0, seg0, mask0, label0 = [], [], [], []\n","\n","  for sample1 in file1:\n","    token1, label1 = [], []\n","    list1 = sample1.split('\\n')\n","\n","    if len(list1) > maxlen-2:\n","      continue\n","\n","    for pair1 in list1:\n","      pair2 = pair1.split(' ')\n","      token1.append(tokenizer.vocab.get(pair2[0], tokenizer.vocab['[UNK]']))\n","      label1.append(label[pair2[1]])\n","\n","    len1 = len(token1)\n","    token0.append([101]+token1+[102]+[0]*(maxlen-len1-2))\n","    seg0.append([0]*maxlen)\n","    mask0.append([0]*(len1+2)+[1]*(maxlen-len1-2))\n","    label0.append([0]+label1+[0]+[0]*(maxlen-len1-2))\n","\n","  token0, seg0, mask0 = np.array(token0), np.array(seg0), np.array(mask0)\n","  label0 = np.eye(len(label.keys()))[label0]\n","  data1 = tf.data.Dataset.from_tensor_slices((token0, seg0, mask0, label0))\n","  return data1.shuffle(len(token0)).batch(batch) if training else data1.batch(batch), len(token0)\n","\n","\n","def label_processing(file, maxlen):\n","  file1, data1 = open(file, encoding='utf-8').read().split('\\n\\n'), []\n","  file1 = [f1 for f1 in file1 if len(f1) > 0]\n","\n","  for i1 in file1:\n","    list1 = [j1.split(' ')[1] for j1 in i1.split('\\n')]\n","\n","    if len(list1) > maxlen-2:\n","      continue\n","    else:\n","      data1.append(list1)\n","\n","  return data1\n","\n","\n","tokenizer_1 = mm.Tokenizer()\n","tokenizer_1.loading(VOCAB)\n","path_1 = 'tasks/datasets/people_daily/'\n","training_1, len_1 = data_processing(path_1+'example.train', tokenizer_1, LABEL, MAXLEN, BATCH, True)\n","dev_1, len_2 = data_processing(path_1+'example.dev', tokenizer_1, LABEL, MAXLEN, BATCH, False)\n","test_1, len_3 = data_processing(path_1+'example.test', tokenizer_1, LABEL, MAXLEN, BATCH, False)\n","devlabel_1 = label_processing(path_1+'example.dev', MAXLEN)\n","testlabel_1 = label_processing(path_1+'example.test', MAXLEN)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RZj2lAHIa05b","colab_type":"code","colab":{}},"source":["class ModelBERT(keras.Model):\n","  def __init__(self, model, config, drop, category, crf=False):\n","    super(ModelBERT, self).__init__()\n","    self.bert = mm.BERT(config, model, 'seq')\n","    self.drop = keras.layers.Dropout(drop)\n","    self.dense = keras.layers.Dense(category, None if crf else 'softmax')\n","    self.crf = mm.CRF(category) if crf else None\n","\n","  def propagating(self, text, segment, mask, training):\n","    x1 = self.bert.propagating(text, segment, mask, training)\n","    return self.dense(self.drop(x1, training=training))\n","\n","\n","class ModelNER(object):\n","  def __init__(self, model, tokenizer, label):\n","    self.tokenizer, self.model = tokenizer, model\n","    self.crf = True if self.model.crf is not None else False\n","    self.vocab = list(self.tokenizer.vocab.keys())\n","    self.map = list(label.keys())\n","\n","  def predicting(self, text, seg, mask):\n","    pred1 = self.model.propagating(text, seg, mask, False)\n","    collection1 = []\n","\n","    if self.crf:\n","      for i1 in self.model.crf.decoding(pred1, mask):\n","        tag1 = [self.map[int(j1)] for j1 in i1]\n","        collection1.append(tag1[1:-1])\n","\n","    else:\n","      for i1, seq1 in enumerate(np.argmax(pred1, 2).tolist()):\n","        mask1 = np.argmax(np.array(mask[i1]).tolist()+[1])\n","        tag1 = [self.map[j1] for j1 in seq1[:mask1]]\n","        collection1.append(tag1[1:-1])\n","\n","    return collection1\n","\n","  def detecting(self, text, seg, mask):\n","    seq1 = self.predicting(text, seg, mask)\n","    collection1 = []\n","\n","    for i1, text1 in enumerate(text):\n","      entity1 = []\n","\n","      for j1, tag1 in enumerate(seq1[i1]):\n","        if tag1 != 'O':\n","          if tag1[0] == 'B':\n","            start1 = True\n","            entity1.append([self.vocab[text1[j1+1]], tag1[2:]])\n","          elif start1:\n","            entity1[-1][0] += self.vocab[text1[j1+1]]\n","          else:\n","            start1 = False\n","          \n","        else:\n","          start1 = False\n","\n","      collection1.append(entity1)\n","\n","    return collection1\n","\n","  def testing(self, sentence, maxlen=64):\n","    text1, seg1, mask1 = self.tokenizer.encoding(sentence, maxlen=maxlen)\n","    return self.detecting(np.array([text1]), np.array([seg1]), np.array([mask1]))\n","\n","\n","model_1 = ModelBERT(MODEL, CONFIG, DROP, len(LABEL.keys()), USECRF)\n","model_1.bert.loading(CKPT)\n","ner_1 = ModelNER(model_1, tokenizer_1, LABEL)\n","function_1 = keras.losses.CategoricalCrossentropy(reduction=keras.losses.Reduction.NONE)\n","loss_1 = tf.keras.metrics.Mean(name='training_loss')\n","step_1 = EPOCH*(len_1/BATCH+1)\n","optimizer_1 = mm.AdamWV2(step_1, LRATE, drate=DRATE)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2c6Pw8RxzFqH","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1597566785547,"user_tz":-480,"elapsed":3814215,"user":{"displayName":"Chris Chen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gib6pz72VKmdLMqZPhKb2UKgIAW_6HhaEfhtM5C=s64","userId":"15295307384944031263"}},"outputId":"48c0ca58-8b10-43de-a86e-934337c04f4e"},"source":["@tf.function\n","def step_training(text, segment, mask, y, crf):\n","  with tf.GradientTape() as tape_1:\n","    pred_1 = model_1.propagating(text, segment, mask, True)\n","\n","    if crf:\n","      cal_1 = model_1.crf.calculating(y, pred_1, mask)\n","      value_1 = tf.reduce_mean(cal_1)\n","    else:\n","      m_1 = tf.cast((1-mask), tf.float32)\n","      value_1 = tf.reduce_sum(function_1(y, pred_1)*m_1)/tf.reduce_sum(m_1)\n","\n","  grad_1 = tape_1.gradient(value_1, model_1.trainable_variables)\n","  grad_1, _ = tf.clip_by_global_norm(grad_1, 1.0)\n","  optimizer_1.apply_gradients(zip(grad_1, model_1.trainable_variables))\n","  loss_1(value_1)\n","\n","\n","def step_evaluating(data, label, model):\n","  pred1 = []\n","\n","  for x1, x2, x3, y1 in data:\n","    pred1 += model.predicting(x1, x2, x3)\n","\n","  return metrics.f1_score(label, pred1), metrics.classification_report(label, pred1)\n","\n","\n","temp_1 = 'Training loss is {:.4f}, and step cost is {:.4f}.'\n","temp_2 = 'Dev F1 score is {:.4f}, and epoch cost is {:.4f}.'\n","count_1 = 0\n","\n","for e_1 in range(EPOCH):\n","  print('Epoch {} running.'.format(e_1+1))\n","  time_0 = time.time()\n","\n","  for x_1, x_2, x_3, y_1 in training_1:\n","    time_1, count_1 = time.time(), count_1+1\n","    step_training(x_1, x_2, x_3, y_1, USECRF)\n","\n","    if count_1 % 200 == 0:\n","      print(temp_1.format(float(loss_1.result()), time.time()-time_1))\n","\n","  f_1, _ = step_evaluating(dev_1, devlabel_1, ner_1)\n","  print(temp_2.format(f_1, time.time()-time_0))\n","  print('**********')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Epoch 1 running.\n","Training loss is 33.5846, and step cost is 0.5706.\n","Training loss is 18.1014, and step cost is 0.5659.\n","Training loss is 12.6950, and step cost is 0.5678.\n","Dev F1 score is 0.9153, and epoch cost is 398.1359.\n","**********\n","Epoch 2 running.\n","Training loss is 9.8677, and step cost is 0.5665.\n","Training loss is 8.1282, and step cost is 0.5592.\n","Training loss is 6.9662, and step cost is 0.5701.\n","Dev F1 score is 0.9273, and epoch cost is 380.0600.\n","**********\n","Epoch 3 running.\n","Training loss is 6.0804, and step cost is 0.5674.\n","Training loss is 5.3927, and step cost is 0.5688.\n","Training loss is 4.8628, and step cost is 0.5644.\n","Dev F1 score is 0.9527, and epoch cost is 379.7818.\n","**********\n","Epoch 4 running.\n","Training loss is 4.4234, and step cost is 0.5675.\n","Training loss is 4.0556, and step cost is 0.5832.\n","Training loss is 3.7500, and step cost is 0.5656.\n","Dev F1 score is 0.9566, and epoch cost is 379.7569.\n","**********\n","Epoch 5 running.\n","Training loss is 3.4889, and step cost is 0.5687.\n","Training loss is 3.2572, and step cost is 0.5650.\n","Training loss is 3.0541, and step cost is 0.5662.\n","Training loss is 2.8810, and step cost is 0.5628.\n","Dev F1 score is 0.9594, and epoch cost is 379.1232.\n","**********\n","Epoch 6 running.\n","Training loss is 2.7184, and step cost is 0.5673.\n","Training loss is 2.5759, and step cost is 0.5703.\n","Training loss is 2.4488, and step cost is 0.5811.\n","Dev F1 score is 0.9627, and epoch cost is 379.8205.\n","**********\n","Epoch 7 running.\n","Training loss is 2.3311, and step cost is 0.5667.\n","Training loss is 2.2253, and step cost is 0.5806.\n","Training loss is 2.1276, and step cost is 0.5605.\n","Dev F1 score is 0.9628, and epoch cost is 379.5075.\n","**********\n","Epoch 8 running.\n","Training loss is 2.0386, and step cost is 0.5655.\n","Training loss is 1.9551, and step cost is 0.5607.\n","Training loss is 1.8797, and step cost is 0.5669.\n","Dev F1 score is 0.9649, and epoch cost is 378.5638.\n","**********\n","Epoch 9 running.\n","Training loss is 1.8096, and step cost is 0.5706.\n","Training loss is 1.7438, and step cost is 0.5692.\n","Training loss is 1.6828, and step cost is 0.5622.\n","Dev F1 score is 0.9625, and epoch cost is 378.7025.\n","**********\n","Epoch 10 running.\n","Training loss is 1.6258, and step cost is 0.5652.\n","Training loss is 1.5723, and step cost is 0.5620.\n","Training loss is 1.5223, and step cost is 0.5641.\n","Training loss is 1.4755, and step cost is 0.5734.\n","Dev F1 score is 0.9654, and epoch cost is 379.8193.\n","**********\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"5msgJ5ySmlwS","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":168},"executionInfo":{"status":"ok","timestamp":1597566838355,"user_tz":-480,"elapsed":32343,"user":{"displayName":"Chris Chen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gib6pz72VKmdLMqZPhKb2UKgIAW_6HhaEfhtM5C=s64","userId":"15295307384944031263"}},"outputId":"1fc31014-0b30-437a-8d8c-9f59bd0d9f64"},"source":["f_1, c_1 = step_evaluating(test_1, testlabel_1, ner_1)\n","print(c_1)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["           precision    recall  f1-score   support\n","\n","      ORG       0.93      0.94      0.93      2066\n","      LOC       0.96      0.96      0.96      3291\n","      PER       0.98      0.98      0.98      1668\n","\n","micro avg       0.95      0.96      0.96      7025\n","macro avg       0.95      0.96      0.96      7025\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"TAk66GYyOnnZ","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1597566846818,"user_tz":-480,"elapsed":785,"user":{"displayName":"Chris Chen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gib6pz72VKmdLMqZPhKb2UKgIAW_6HhaEfhtM5C=s64","userId":"15295307384944031263"}},"outputId":"24aaceda-902a-4dcc-fcf6-8975219a6ed4"},"source":["ner_1.testing('陈旺财已经从匹兹堡回到上海了。')[0]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[['陈旺财', 'PER'], ['匹兹堡', 'LOC'], ['上海', 'LOC']]"]},"metadata":{"tags":[]},"execution_count":7}]}]}