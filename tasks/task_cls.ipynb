{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"task_cls.ipynb（副本）","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"toQJl6Q4xjDp","colab_type":"text"},"source":["A text classification example for BERT using Google Colab.  \n","The data is from https://github.com/FudanNLP/nlpcc2017_news_headline_categorization.\n"]},{"cell_type":"code","metadata":{"id":"9TsxfYM1NkM2","colab_type":"code","colab":{}},"source":["try:\n","  %tensorflow_version 2.x\n","except Exception:\n","  pass\n","\n","import time\n","import json\n","import numpy as np\n","import pandas as pd\n","import tensorflow as tf\n","import tensorflow.keras as keras\n","from sklearn import preprocessing\n","from bert_by_tf2 import BERT, AdamW, Tokenizer"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ik9wS_1LPBlC","colab_type":"code","outputId":"91e752d7-7380-483b-9d3b-00469a4d079e","executionInfo":{"status":"ok","timestamp":1583636749224,"user_tz":-480,"elapsed":9970,"user":{"displayName":"Chris Chen","photoUrl":"","userId":"15295307384944031263"}},"colab":{"base_uri":"https://localhost:8080/","height":118}},"source":["trai_1 = pd.read_table('datasets/a/train.txt', header=None, names=['label', 'text'])\n","deve_1 = pd.read_table('datasets/a/dev.txt', header=None, names=['label', 'text'])\n","test_1 = pd.read_table('datasets/a/test.txt', header=None, names=['label', 'text'])\n","print(test_1.head())"],"execution_count":0,"outputs":[{"output_type":"stream","text":["           label                                            text\n","0           baby                 生完 小孩 ， 公公 伺候 我 坐月子 ， 很 羞涩 很 感动\n","1        fashion    唐艺昕 与 陈伟霆 为 初秋 的 情侣 做出 了 典范 ， 看 他们 如何 穿 情侣 衣\n","2  entertainment  同学聚会 美女 被 嘲笑 是 剩女 ， 当超帅 老公 带 着 儿子 出场 ， 全场 沸腾 了\n","3        finance                             中国 供给 侧 至少 存在 六大 问题\n","4          world                                2.5 万英镑 可住 戴妃 闺房\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"mT-9Ky25bay2","colab_type":"code","outputId":"713b083e-af1c-43f9-883c-fac9aad13caa","executionInfo":{"status":"ok","timestamp":1583636753725,"user_tz":-480,"elapsed":834,"user":{"displayName":"Chris Chen","photoUrl":"","userId":"15295307384944031263"}},"colab":{"base_uri":"https://localhost:8080/","height":319}},"source":["labe_1 = preprocessing.LabelEncoder()\n","trai_1['label'] = labe_1.fit_transform(trai_1['label'])\n","deve_1['label'] = labe_1.transform(deve_1['label'])\n","test_1['label'] = labe_1.transform(test_1['label'])\n","\n","for i in range(18):\n","    print(i, labe_1.inverse_transform([i])[0])"],"execution_count":0,"outputs":[{"output_type":"stream","text":["0 baby\n","1 car\n","2 discovery\n","3 entertainment\n","4 essay\n","5 fashion\n","6 finance\n","7 food\n","8 game\n","9 history\n","10 military\n","11 regimen\n","12 society\n","13 sports\n","14 story\n","15 tech\n","16 travel\n","17 world\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"4B2HpNZ7b9N7","colab_type":"code","colab":{}},"source":["ALBERT = False\n","MAXLEN = 40\n","CATE = 18\n","DROP = 0.5\n","DIM = 128\n","LRATE = 5e-5\n","BATCH = 64\n","EPOCH = 3\n","\n","VOCAB = 'models/bert_base_ch/vocab.txt'\n","CONFIG = 'models/bert_base_ch/bert_config.json'\n","CKPT = 'models/bert_base_ch/bert_model.ckpt'"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"cXuiXbNfYCuY","colab_type":"code","colab":{}},"source":["def data_processing(data, tokenizer, maxlen, batch):\n","    text1, type1, mask1, labe1 = [], [], [], []\n","\n","    for i in range(len(data)):\n","        text2, type2, mask2 = tokenizer.encoding(data['text'][i], None, maxlen)\n","        labe2 = data['label'][i]\n","        text1.append(text2)\n","        type1.append(type2)\n","        mask1.append(mask2)\n","        labe1.append(labe2)\n","\n","    text1, type1, mask1, labe1 = np.array(text1), np.array(type1), np.array(mask1), np.array(labe1)\n","    return tf.data.Dataset.from_tensor_slices((text1, type1, mask1, labe1)).shuffle(len(text1)).batch(batch)\n","\n","toke_1 = Tokenizer()\n","toke_1.loading(VOCAB)\n","trai_2 = data_processing(trai_1, toke_1, MAXLEN, BATCH)\n","deve_2 = data_processing(deve_1, toke_1, MAXLEN, BATCH)\n","test_2 = data_processing(test_1, toke_1, MAXLEN, BATCH)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"gzXMCDSad3rV","colab_type":"code","colab":{}},"source":["class MyModel(keras.Model):\n","  def __init__(self, albert, config, drop, dim, category):\n","    super(MyModel, self).__init__()\n","    self.bert = BERT(config, albert)\n","    self.drop = keras.layers.Dropout(drop)\n","    self.dense1 = keras.layers.Dense(dim, activation='relu')\n","    self.dense2 = keras.layers.Dense(category, activation='softmax')\n","\n","  def propagating(self, text, segment, mask, training):\n","    x1 = self.bert.propagating(text, segment, mask, True, training)\n","    x1 = self.drop(x1, training=training)\n","    return self.dense2(self.dense1(x1))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"QRai4oFr-qtx","colab_type":"code","colab":{}},"source":["step_1 = EPOCH*(int(len(trai_1)/BATCH)+1)\n","loss_1 = keras.losses.SparseCategoricalCrossentropy()\n","opti_1 = AdamW(step_1, LRATE)\n","mode_1 = MyModel(ALBERT, CONFIG, DROP, DIM, CATE)\n","mode_1.bert.loading(CKPT)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"RxHmRINF9R2x","colab_type":"code","colab":{}},"source":["l_1 = tf.keras.metrics.Mean(name='training_loss')\n","a_1 = tf.keras.metrics.SparseCategoricalAccuracy(name='training_accuracy')\n","l_2 = tf.keras.metrics.Mean(name='test_loss')\n","a_2 = tf.keras.metrics.SparseCategoricalAccuracy(name='test_accuracy')\n","coun_1 = 0\n","\n","@tf.function\n","def step_training(text, segment, mask, y):\n","  with tf.GradientTape() as tape:\n","    pred1 = mode_1.propagating(text, segment, mask, True)\n","    loss1 = loss_1(y, pred1)\n","\n","  grad_1 = tape.gradient(loss1, mode_1.trainable_variables)\n","  opti_1.apply_gradients(zip(grad_1, mode_1.trainable_variables))\n","  l_1(loss1)\n","  a_1(y, pred1)\n","\n","@tf.function\n","def step_evaluating(text, segment, mask, y):\n","  pred1 = mode_1.propagating(text, segment, mask, False)\n","  loss1 = loss_1(y, pred1)\n","  l_2(loss1)\n","  a_2(y, pred1)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"pFLOn6g1--aM","colab_type":"code","outputId":"52b63885-a1ab-4972-fe1a-cfe39f953cc4","executionInfo":{"status":"ok","timestamp":1583640876112,"user_tz":-480,"elapsed":866937,"user":{"displayName":"Chris Chen","photoUrl":"","userId":"15295307384944031263"}},"colab":{"base_uri":"https://localhost:8080/","height":353}},"source":["temp_1 = 'Epoch {} running, loss is {}, training accuracy is {}, and step cost is {}.'\n","temp_2 = 'Epoch {} completed, training accuracy is {}, and test accuracy is {}.'\n","\n","for e_1 in range(EPOCH):\n","  for x_1, x_2, x_3, y_1 in trai_2:\n","    time_1, coun_1 = time.time(), coun_1+1\n","    step_training(x_1, x_2, x_3, y_1)\n","\n","    if coun_1 % 500 == 0:\n","        o_1, o_2 = round(float(l_1.result()), 4), round(float(a_1.result()), 4)\n","        print(temp_1.format(e_1+1, o_1, o_2, round(time.time()-time_1, 4)))\n","\n","  for x_1, x_2, x_3, y_1 in deve_2:\n","    step_evaluating(x_1, x_2, x_3, y_1)\n","\n","  print(temp_2.format(e_1+1, round(float(a_1.result()), 4), round(float(a_2.result()), 4)))\n","  print('**********')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Epoch 1 running, loss is 1.1703, training accuracy is 0.6733, and step cost is 0.4859.\n","Epoch 1 running, loss is 0.9623, training accuracy is 0.7297, and step cost is 0.4908.\n","Epoch 1 running, loss is 0.872, training accuracy is 0.7527, and step cost is 0.5247.\n","Epoch 1 running, loss is 0.8187, training accuracy is 0.7657, and step cost is 0.488.\n","Epoch 1 completed, training accuracy is 0.7741, and test accuracy is 0.8208.\n","**********\n","Epoch 2 running, loss is 0.7765, training accuracy is 0.7764, and step cost is 0.5035.\n","Epoch 2 running, loss is 0.7239, training accuracy is 0.7908, and step cost is 0.4829.\n","Epoch 2 running, loss is 0.686, training accuracy is 0.8009, and step cost is 0.4836.\n","Epoch 2 running, loss is 0.6572, training accuracy is 0.8087, and step cost is 0.4741.\n","Epoch 2 running, loss is 0.6338, training accuracy is 0.8152, and step cost is 0.4752.\n","Epoch 2 completed, training accuracy is 0.8191, and test accuracy is 0.8274.\n","**********\n","Epoch 3 running, loss is 0.6111, training accuracy is 0.8217, and step cost is 0.5227.\n","Epoch 3 running, loss is 0.5806, training accuracy is 0.8303, and step cost is 0.4799.\n","Epoch 3 running, loss is 0.5557, training accuracy is 0.8375, and step cost is 0.5221.\n","Epoch 3 running, loss is 0.5348, training accuracy is 0.8435, and step cost is 0.4864.\n","Epoch 3 running, loss is 0.5153, training accuracy is 0.849, and step cost is 0.5227.\n","Epoch 3 completed, training accuracy is 0.8519, and test accuracy is 0.8304.\n","**********\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"MGfdDPRigKDR","colab_type":"code","outputId":"bf4a5722-e1e1-4de6-a718-51065105d45c","executionInfo":{"status":"ok","timestamp":1583641001476,"user_tz":-480,"elapsed":120470,"user":{"displayName":"Chris Chen","photoUrl":"","userId":"15295307384944031263"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["c_1, c_2 = 0, 0\n","\n","for x_1, x_2, x_3, y_1 in test_2:\n","  pred_1 = mode_1.propagating(x_1, x_2, x_3, False)\n","  comp_1 = sum(np.array(y_1)==np.argmax(pred_1, 1))\n","  c_1, c_2 = c_1+len(pred_1), c_2+comp_1\n","\n","print('Test accuracy is '+str(c_2/c_1)+'.')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Test accuracy is 0.832239951075777.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"pCpx0BLzXd47","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}