{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"text_classification.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"toQJl6Q4xjDp","colab_type":"text"},"source":["A text classification example for BERT.  \n","The data is from https://github.com/FudanNLP/nlpcc2017_news_headline_categorization."]},{"cell_type":"code","metadata":{"id":"WWnNxbk7Zokr","colab_type":"code","colab":{}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rBlT22miZU8c","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1599567695886,"user_tz":-480,"elapsed":2753,"user":{"displayName":"Chris Chen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gib6pz72VKmdLMqZPhKb2UKgIAW_6HhaEfhtM5C=s64","userId":"15295307384944031263"}}},"source":["%tensorflow_version 2.x\n","\n","import os\n","import warnings\n","import time\n","import json\n","import numpy as np\n","import pandas as pd\n","import tensorflow as tf\n","import tensorflow.keras as keras\n","from sklearn import preprocessing\n","\n","os.chdir('./drive/My Drive/Python/Research/bert')\n","warnings.filterwarnings('ignore')\n","\n","import mymodels as mm"],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"id":"NbYyoo5qaVGU","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1599567697393,"user_tz":-480,"elapsed":1086,"user":{"displayName":"Chris Chen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gib6pz72VKmdLMqZPhKb2UKgIAW_6HhaEfhtM5C=s64","userId":"15295307384944031263"}}},"source":["MODEL = 'bert'\n","MODE = 'cls'\n","MAXLEN = 40\n","CATE = 18\n","DROP = 0.5\n","LRATE = 5e-5\n","BATCH = 64\n","EPOCH = 3\n","VOCAB = 'models/bert_base_ch/vocab.txt'\n","CONFIG = 'models/bert_base_ch/bert_config.json'\n","CKPT = 'models/bert_base_ch/bert_model.ckpt'"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"cXuiXbNfYCuY","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":118},"executionInfo":{"status":"ok","timestamp":1599567735512,"user_tz":-480,"elapsed":36856,"user":{"displayName":"Chris Chen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gib6pz72VKmdLMqZPhKb2UKgIAW_6HhaEfhtM5C=s64","userId":"15295307384944031263"}},"outputId":"74365cd9-c255-48cb-e73b-71b56c6ee94b"},"source":["def data_processing(data, tokenizer, maxlen, batch, training):\n","  text1, seg1, mask1, label1 = [], [], [], []\n","  \n","  for i in range(len(data)):\n","    text2, seg2, mask2 = tokenizer.encoding(data['text'][i], None, maxlen)\n","    text1.append(text2)\n","    seg1.append(seg2)\n","    mask1.append(mask2)\n","    label1.append(data['label'][i])\n","    \n","  text1, seg1, mask1, label1 = np.array(text1), np.array(seg1), np.array(mask1), np.array(label1)\n","  data1 = tf.data.Dataset.from_tensor_slices((text1, seg1, mask1, label1))\n","  return data1.shuffle(len(text1)).batch(batch) if training else data1.batch(batch)\n","\n","\n","training_1 = pd.read_table('tasks/datasets/nlpcc_2017_news/train.txt', names=['label', 'text'])\n","dev_1 = pd.read_table('tasks/datasets/nlpcc_2017_news/dev.txt', names=['label', 'text'])\n","test_1 = pd.read_table('tasks/datasets/nlpcc_2017_news/test.txt', names=['label', 'text'])\n","\n","label_1 = preprocessing.LabelEncoder()\n","training_1['label'] = label_1.fit_transform(training_1['label'])\n","dev_1['label'] = label_1.transform(dev_1['label'])\n","test_1['label'] = label_1.transform(test_1['label'])\n","\n","tokenizer_1 = mm.Tokenizer()\n","tokenizer_1.loading(VOCAB)\n","training_2 = data_processing(training_1, tokenizer_1, MAXLEN, BATCH, True)\n","dev_2 = data_processing(dev_1, tokenizer_1, MAXLEN, BATCH, False)\n","test_2 = data_processing(test_1, tokenizer_1, MAXLEN, BATCH, False)\n","\n","print(training_1.head())"],"execution_count":3,"outputs":[{"output_type":"stream","text":["   label                                               text\n","0      3      台 媒 预 测 周 冬 雨 金 马 奖 封 后 ， 大 气 的 倪 妮 却 佳 作 难 出\n","1      7  农 村 就 是 好 ， 能 吃 到 纯 天 然 无 添 加 的 野 生 蜂 蜜 ， 营 养 ...\n","2      5        1 4 款 知 性 美 装 ， 时 尚 惊 艳 搁 浅 的 阳 光 轻 熟 的 优 雅\n","3      9              火 焰 喷 射 器 1 0 0 0 度 火 焰 烧 死 鬼 子 4 连 拍\n","4     12                            1 8 岁 青 年 砍 死 8 8 岁 老 兵\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"RZj2lAHIa05b","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1599567740336,"user_tz":-480,"elapsed":3263,"user":{"displayName":"Chris Chen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gib6pz72VKmdLMqZPhKb2UKgIAW_6HhaEfhtM5C=s64","userId":"15295307384944031263"}}},"source":["class MyModel(keras.Model):\n","  def __init__(self, model, mode, config, drop, category):\n","    super(MyModel, self).__init__()\n","    self.bert = mm.BERT(config, model, mode)\n","    self.drop = keras.layers.Dropout(drop)\n","    self.dense = keras.layers.Dense(category, activation='softmax')\n","\n","  def propagating(self, text, segment, mask, training):\n","    x1 = self.bert.propagating(text, segment, mask, training)\n","    return self.dense(self.drop(x1, training=training))\n","\n","\n","model_1 = MyModel(MODEL, MODE, CONFIG, DROP, CATE)\n","model_1.bert.loading(CKPT)\n","function_1 = keras.losses.SparseCategoricalCrossentropy()\n","optimizer_1 = mm.AdamWV2(EPOCH*(int(len(training_1)/BATCH)+1), LRATE)\n","\n","loss_1 = tf.keras.metrics.Mean(name='training_loss')\n","acc_1 = tf.keras.metrics.SparseCategoricalAccuracy(name='training_accuracy')\n","acc_2 = tf.keras.metrics.SparseCategoricalAccuracy(name='dev_accuracy')"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"3-kZtSX8ecC4","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":403},"executionInfo":{"status":"ok","timestamp":1599570513694,"user_tz":-480,"elapsed":2771688,"user":{"displayName":"Chris Chen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gib6pz72VKmdLMqZPhKb2UKgIAW_6HhaEfhtM5C=s64","userId":"15295307384944031263"}},"outputId":"398ad169-20d6-4c69-8a1d-0a0d17674374"},"source":["@tf.function\n","def step_training(text, segment, mask, y):\n","  with tf.GradientTape() as tape_1:\n","    pred_1 = model_1.propagating(text, segment, mask, True)\n","    value_1 = function_1(y, pred_1)\n","\n","  grad_1 = tape_1.gradient(value_1, model_1.trainable_variables)\n","  grad_1, _ = tf.clip_by_global_norm(grad_1, 1.0)\n","  optimizer_1.apply_gradients(zip(grad_1, model_1.trainable_variables))\n","  loss_1(value_1)\n","  acc_1(y, pred_1)\n","\n","\n","@tf.function\n","def step_evaluating(text, segment, mask, y):\n","  pred_1 = model_1.propagating(text, segment, mask, False)\n","  acc_2(y, pred_1)\n","\n","\n","temp_1 = 'Training loss is {:.4f}, accuracy is {:.4f}.'\n","temp_2 = 'Dev accuracy is {:.4f}, and epoch cost is {:.4f}.'\n","count_1 = 0\n","\n","for e_1 in range(EPOCH):\n","  print('Epoch {} running.'.format(e_1+1))\n","  time_0 = time.time()\n","\n","  for x_1, x_2, x_3, y_1 in training_2:\n","    time_1, count_1 = time.time(), count_1+1\n","    step_training(x_1, x_2, x_3, y_1)\n","\n","    if count_1 % 500 == 0:\n","      print(temp_1.format(float(loss_1.result()), float(acc_1.result())))\n","\n","  for x_1, x_2, x_3, y_1 in dev_2:\n","    step_evaluating(x_1, x_2, x_3, y_1)\n","  \n","  print(temp_2.format(float(acc_2.result()), time.time()-time_0))\n","  print('**********')\n","  acc_1.reset_states()\n","  acc_2.reset_states()"],"execution_count":5,"outputs":[{"output_type":"stream","text":["Epoch 1 running.\n","Training loss is 1.5211, accuracy is 0.5865.\n","Training loss is 1.1668, accuracy is 0.6815.\n","Training loss is 1.0240, accuracy is 0.7184.\n","Training loss is 0.9330, accuracy is 0.7412.\n","Dev accuracy is 0.8236, and epoch cost is 931.2976.\n","**********\n","Epoch 2 running.\n","Training loss is 0.8732, accuracy is 0.8466.\n","Training loss is 0.8110, accuracy is 0.8523.\n","Training loss is 0.7622, accuracy is 0.8565.\n","Training loss is 0.7261, accuracy is 0.8576.\n","Training loss is 0.6960, accuracy is 0.8594.\n","Dev accuracy is 0.8395, and epoch cost is 920.6781.\n","**********\n","Epoch 3 running.\n","Training loss is 0.6677, accuracy is 0.9068.\n","Training loss is 0.6345, accuracy is 0.9085.\n","Training loss is 0.6063, accuracy is 0.9097.\n","Training loss is 0.5819, accuracy is 0.9115.\n","Training loss is 0.5605, accuracy is 0.9124.\n","Dev accuracy is 0.8410, and epoch cost is 918.4990.\n","**********\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"PpxbxpliLm02","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1599570582860,"user_tz":-480,"elapsed":59488,"user":{"displayName":"Chris Chen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gib6pz72VKmdLMqZPhKb2UKgIAW_6HhaEfhtM5C=s64","userId":"15295307384944031263"}},"outputId":"5e2198c5-932f-4f73-fb02-0f66b0657493"},"source":["for x_1, x_2, x_3, y_1 in test_2:\n","  step_evaluating(x_1, x_2, x_3, y_1)\n","\n","print('Test accuracy is {:.4f}.'.format(float(acc_2.result())))"],"execution_count":6,"outputs":[{"output_type":"stream","text":["Test accuracy is 0.8407.\n"],"name":"stdout"}]}]}