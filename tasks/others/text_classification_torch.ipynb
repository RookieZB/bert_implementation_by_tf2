{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"text_classification_torch.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"k3oEHR4piBQc","colab_type":"text"},"source":["A text classification example for BERT by PyTorch.  \n","The data is from https://github.com/FudanNLP/nlpcc2017_news_headline_categorization."]},{"cell_type":"code","metadata":{"id":"WWnNxbk7Zokr","colab_type":"code","colab":{}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rBlT22miZU8c","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1596288197355,"user_tz":-480,"elapsed":3040,"user":{"displayName":"Chris Chen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gib6pz72VKmdLMqZPhKb2UKgIAW_6HhaEfhtM5C=s64","userId":"15295307384944031263"}}},"source":["import os\n","import warnings\n","import time\n","import torch\n","import numpy as np\n","import pandas as pd\n","from sklearn import preprocessing\n","from torch import utils\n","from transformers import BertTokenizer, BertModel, AdamW, get_linear_schedule_with_warmup\n","\n","os.chdir('./drive/My Drive/Python/Research/bert')\n","warnings.filterwarnings('ignore')"],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"id":"NbYyoo5qaVGU","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1596288202139,"user_tz":-480,"elapsed":879,"user":{"displayName":"Chris Chen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gib6pz72VKmdLMqZPhKb2UKgIAW_6HhaEfhtM5C=s64","userId":"15295307384944031263"}}},"source":["MAXLEN = 40\n","CATE = 18\n","DROP = 0.5\n","LRATE = 5e-5\n","BATCH = 64\n","EPOCH = 3\n","MODEL = 'bert-base-chinese'\n","DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"ik9wS_1LPBlC","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":118},"executionInfo":{"status":"ok","timestamp":1596288207979,"user_tz":-480,"elapsed":3034,"user":{"displayName":"Chris Chen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gib6pz72VKmdLMqZPhKb2UKgIAW_6HhaEfhtM5C=s64","userId":"15295307384944031263"}},"outputId":"305ce8d8-02c1-41be-fc5d-b2f57eed96e9"},"source":["path_1 = 'tasks/datasets/nlpcc_2017_news/'\n","training_1 = pd.read_table(path_1+'train.txt', names=['label', 'text'])\n","dev_1 = pd.read_table(path_1+'dev.txt', names=['label', 'text'])\n","test_1 = pd.read_table(path_1+'test.txt', names=['label', 'text'])\n","\n","encoder_1 = preprocessing.LabelEncoder()\n","training_1['label'] = encoder_1.fit_transform(training_1['label'])\n","dev_1['label'] = encoder_1.transform(dev_1['label'])\n","test_1['label'] = encoder_1.transform(test_1['label'])\n","\n","print(training_1.head())"],"execution_count":3,"outputs":[{"output_type":"stream","text":["   label                                               text\n","0      3      台 媒 预 测 周 冬 雨 金 马 奖 封 后 ， 大 气 的 倪 妮 却 佳 作 难 出\n","1      7  农 村 就 是 好 ， 能 吃 到 纯 天 然 无 添 加 的 野 生 蜂 蜜 ， 营 养 ...\n","2      5        1 4 款 知 性 美 装 ， 时 尚 惊 艳 搁 浅 的 阳 光 轻 熟 的 优 雅\n","3      9              火 焰 喷 射 器 1 0 0 0 度 火 焰 烧 死 鬼 子 4 连 拍\n","4     12                            1 8 岁 青 年 砍 死 8 8 岁 老 兵\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"n85XI1QRUc4v","colab_type":"code","colab":{}},"source":["class DataProcessor(utils.data.Dataset):\n","  def __init__(self, dataframe, tokenizer, maxlen):\n","    self.len = len(dataframe)\n","    self.data = dataframe\n","    self.tokenizer = tokenizer\n","    self.maxlen = maxlen\n","\n","  def __len__(self):\n","    return self.len\n","\n","  def __getitem__(self, index):\n","    text = ' '.join(str(self.data.text[index]).split())\n","    inputs = self.tokenizer.encode_plus(\n","      text,\n","      None,\n","      add_special_tokens=True,\n","      max_length=self.maxlen,\n","      pad_to_max_length=True,\n","      return_token_type_ids=True,\n","      truncation=True)\n","    return {\n","      'ids': torch.tensor(inputs['input_ids'], dtype=torch.long),\n","      'mask': torch.tensor(inputs['attention_mask'], dtype=torch.long),\n","      'targets': torch.tensor(self.data.label[index], dtype=torch.long)}\n","\n","\n","tokenizer_1 = BertTokenizer.from_pretrained(MODEL)\n","param_1 = {'batch_size': BATCH, 'shuffle': True, 'num_workers': 0}\n","training_2 = utils.data.DataLoader(DataProcessor(training_1, tokenizer_1, MAXLEN), **param_1)\n","dev_2 = utils.data.DataLoader(DataProcessor(dev_1, tokenizer_1, MAXLEN), **param_1)\n","test_2 = utils.data.DataLoader(DataProcessor(test_1, tokenizer_1, MAXLEN), **param_1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"iyzBaUHIUc16","colab_type":"code","colab":{}},"source":["class ModelCLS(torch.nn.Module):\n","  def __init__(self, ckpt, drop, cate):\n","    super(ModelCLS, self).__init__()\n","    self.bert = BertModel.from_pretrained(ckpt)\n","    self.drop = torch.nn.Dropout(drop)\n","    self.dense = torch.nn.Linear(768, cate)\n","    \n","  def forward(self, ids, mask):\n","    _, x = self.bert(ids, attention_mask=mask)\n","    return self.dense(self.drop(x))\n","\n","\n","model_1 = ModelCLS(MODEL, DROP, CATE)\n","model_1.to(DEVICE)\n","nodecay_1 = ['bias', 'LayerNorm.weight']\n","var_1 = [\n","  {'params': [p_1 for n_1, p_1 in model_1.named_parameters() if not any(\n","    nd_1 in n_1 for nd_1 in nodecay_1)], 'weight_decay': 0.01},\n","  {'params': [p_1 for n_1, p_1 in model_1.named_parameters() if any(\n","    nd_1 in n_1 for nd_1 in nodecay_1)], 'weight_decay': 0.00}]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OuiikRvaUcw_","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1596288350619,"user_tz":-480,"elapsed":766,"user":{"displayName":"Chris Chen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gib6pz72VKmdLMqZPhKb2UKgIAW_6HhaEfhtM5C=s64","userId":"15295307384944031263"}}},"source":["step_1 = EPOCH*(int(len(training_1)/BATCH)+1)\n","function_1 = torch.nn.CrossEntropyLoss()\n","optimizer_1 = AdamW(var_1, lr=LRATE)\n","sch_1 = get_linear_schedule_with_warmup(optimizer_1, int(step_1*0.1), step_1)\n","record_1 = []\n","\n","\n","def step_training(data):\n","  model_1.train()\n","  total, correct = 0, 0\n","\n","  for r, data in enumerate(data, 0):\n","    ids = data['ids'].to(DEVICE, dtype=torch.long)\n","    mask = data['mask'].to(DEVICE, dtype=torch.long)\n","    targets = data['targets'].to(DEVICE, dtype=torch.long)\n","    outputs = model_1(ids, mask).squeeze()\n","    loss = function_1(outputs, targets)\n","\n","    total += targets.shape[0]\n","    correct += (outputs.argmax(-1) == targets).sum().item()\n","    record_1.append(loss.item())\n","    \n","    optimizer_1.zero_grad()\n","    loss.backward()\n","    optimizer_1.step()\n","    sch_1.step()\n","\n","    if (r+1) % 500 == 0:\n","      totalloss = round(np.mean(record_1), 4)\n","      acc = round(correct/total, 4)\n","      print(f'Training loss is {totalloss}, and accuracy is {acc}.')\n","\n","\n","def step_evaluating(data):\n","  model_1.eval()\n","  total, correct = 0, 0\n","\n","  for _, data in enumerate(data, 0):\n","    ids = data['ids'].to(DEVICE, dtype=torch.long)\n","    mask = data['mask'].to(DEVICE, dtype=torch.long)\n","    targets = data['targets'].to(DEVICE, dtype=torch.long)\n","    outputs = model_1(ids, mask).squeeze()\n","\n","    total += targets.shape[0]\n","    correct += (outputs.argmax(-1) == targets).sum().item()\n","\n","  acc = round(correct/total, 4)\n","  print(f'Test (dev) accuracy is {acc}.')"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"Fqc-dNo-UcuO","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":420},"executionInfo":{"status":"ok","timestamp":1596290598479,"user_tz":-480,"elapsed":2243071,"user":{"displayName":"Chris Chen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gib6pz72VKmdLMqZPhKb2UKgIAW_6HhaEfhtM5C=s64","userId":"15295307384944031263"}},"outputId":"0f6e4351-14b7-4e9e-8da0-64de1486d97d"},"source":["for epoch_1 in range(EPOCH):\n","  print('Epoch {} running.'.format(epoch_1+1))\n","  check_1 = time.time()\n","  step_training(training_2)\n","  step_evaluating(dev_2)\n","  print('Epoch time cost is {}.'.format(round(time.time()-check_1, 4)))\n","  print('**********')"],"execution_count":7,"outputs":[{"output_type":"stream","text":["Epoch 1 running.\n","Training loss is 1.4872, and accuracy is 0.5916.\n","Training loss is 1.1076, and accuracy is 0.6926.\n","Training loss is 0.9638, and accuracy is 0.7296.\n","Training loss is 0.8837, and accuracy is 0.7504.\n","Test (dev) accuracy is 0.8255.\n","Epoch time cost is 748.874.\n","**********\n","Epoch 2 running.\n","Training loss is 0.7747, and accuracy is 0.8613.\n","Training loss is 0.7304, and accuracy is 0.8607.\n","Training loss is 0.6976, and accuracy is 0.8601.\n","Training loss is 0.6707, and accuracy is 0.861.\n","Test (dev) accuracy is 0.8349.\n","Epoch time cost is 747.1922.\n","**********\n","Epoch 3 running.\n","Training loss is 0.6184, and accuracy is 0.9085.\n","Training loss is 0.5913, and accuracy is 0.91.\n","Training loss is 0.5678, and accuracy is 0.9116.\n","Training loss is 0.547, and accuracy is 0.9128.\n","Test (dev) accuracy is 0.8387.\n","Epoch time cost is 746.1158.\n","**********\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"kuFN8QyElmN3","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1596290686191,"user_tz":-480,"elapsed":60229,"user":{"displayName":"Chris Chen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gib6pz72VKmdLMqZPhKb2UKgIAW_6HhaEfhtM5C=s64","userId":"15295307384944031263"}},"outputId":"39e0a969-1eb2-4ea5-ca7d-9d11291118f4"},"source":["step_evaluating(test_2)"],"execution_count":8,"outputs":[{"output_type":"stream","text":["Test (dev) accuracy is 0.8373.\n"],"name":"stdout"}]}]}